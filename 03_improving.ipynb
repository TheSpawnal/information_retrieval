{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Improving the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will improve the search index and query functions from the previous assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Defining Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is copied from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle, bz2, re\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from IPython.display import display, HTML\n",
    "from math import log10, sqrt\n",
    "\n",
    "Summaries_file = 'data/emotion_Summaries.pkl.bz2'\n",
    "Abstracts_file = 'data/emotion_Abstracts.pkl.bz2'\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on a list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_summary( id, show_abstract=False, show_id=True, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long author lists, and adds a link to the paper's DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[id]\n",
    "    lines = []\n",
    "    title = s.title\n",
    "    if s.doi != '':\n",
    "        title = '<a href=http://dx.doi.org/{:s}>{:s}</a>'.format(s.doi, title)\n",
    "    title = '<strong>' + title + '</strong>'\n",
    "    lines.append(title)\n",
    "    authors = ', '.join( s.authors[:20] ) + ('' if len(s.authors) <= 20 else ', ...')\n",
    "    lines.append(str(s.year) + '. ' + authors)\n",
    "    if (show_abstract):\n",
    "        lines.append('<small><strong>Abstract:</strong> <em>{:s}</em></small>'.format(Abstracts[id]))\n",
    "    if (show_id):\n",
    "        lines.append('[ID: {:d}]'.format(id))\n",
    "    if (extra_text != ''):\n",
    "         lines.append(extra_text)\n",
    "    display( HTML('<br>'.join(lines)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for id in sorted(Summaries.keys()):\n",
    "    term_set = set(preprocess(tokenize(Summaries[id].title)))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(preprocess(tokenize(Abstracts[id])))\n",
    "    for term in term_set:\n",
    "        inverted_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see from the results of the last assignment, our simple index doesn't handle punctuation and the difference between singular and plural versions of the same word very well. We won't go much into the details of tokenization and linguistic analysis here, because we also want to focus on scoring and ranking below. Therefore, we are using an existing library for tokenizatoin and stemming, namely the NLTK package. The following line will install NLTK if necessary (or you have to follow [these instructions](http://www.nltk.org/install.html) if that doesn't work):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/spawn_delta/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/spawn_delta/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: joblib in /home/spawn_delta/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/spawn_delta/.local/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: click in /home/spawn_delta/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "  Good muffins cost $3.88\n",
      "in New York.  Please buy me two of them.\n",
      "\n",
      "Thanks.\n",
      "TOKENIZE:  ['Good', 'muffins', 'cost', '$3.88\\nin', 'New', 'York.', '', 'Please', 'buy', 'me', 'two', 'of', 'them.\\n\\nThanks.']\n",
      "WORD TOKENIZE:  ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
    "\n",
    "print('INPUT TEXT:\\n ', s)\n",
    "\n",
    "print('TOKENIZE: ', tokenize(s))\n",
    "print('WORD TOKENIZE: ', word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"processes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important method to improve our search results is to rank them, which can be done by calculating a score for each document based on the matching terms from the query. One such scoring method is *tf-idf*, which comes with several variants, as explained in the lecture slides.\n",
    "\n",
    "In order to quickly calculate the scores for a term/document combination, we'll need quick access to a couple of things:\n",
    "\n",
    "- tf(t,d): How often does a term occur in a document\n",
    "- df(t): In how many documents does a term occur\n",
    "- num_documents: The number of documents in our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_matrix = defaultdict(Counter)\n",
    "\n",
    "for doc_id in Summaries.keys():\n",
    "    tokens = preprocess(tokenize(Summaries[doc_id].title))\n",
    "    if (doc_id in Abstracts):\n",
    "        tokens.extend(preprocess(tokenize(Abstracts[doc_id])))\n",
    "    tf_matrix[doc_id] = Counter(tokens)\n",
    "\n",
    "def tf(t,d):\n",
    "    return float(tf_matrix[d][t])\n",
    "\n",
    "def df(t):\n",
    "    return float(len(inverted_index[t]))\n",
    "\n",
    "num_documents = float(len(Summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "716.0\n",
      "46483.0\n"
     ]
    }
   ],
   "source": [
    "print(tf('music', 33269144))\n",
    "print(df('music'))\n",
    "print(num_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these helper functions, we can now easily calculate the _tf-idf_ weights of a term in a document by implementing the weighting formula from the slides, which you will do in the assignments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** Aldric de Jacquelin #2711498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement in the code block below the `smart_tokenize_and_preprocess` function using NLTK's functions for tokenization and stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'about', 'inform', 'retriev', '(', 'ir', ')', 'etc', '.', 'cost', 'at', 'least', '$', '25.00', '!']\n"
     ]
    }
   ],
   "source": [
    "# Smarter linguistic processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "def smart_tokenize_and_preprocess(text):\n",
    "    stemmer = EnglishStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    preprocessed_tokens = [stemmer.stem(token.lower()) for token in tokens]\n",
    "    return preprocessed_tokens\n",
    "\n",
    "test1_text = \"Books about Information Retrieval (IR) etc. cost at least $25.00!\"\n",
    "print(smart_tokenize_and_preprocess(test1_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a smarter index based on this function. For practical purposes, the code below generates the smarter index on a subset of the data, as generating an index with stemming on the entire set would take too much time. (You don't need to change or add anything in the code block below. Just leave it as it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Below, we create our smarter index (based on a subset of the documents for demonstration purposes)\n",
    "smarter_index = defaultdict(list)\n",
    "\n",
    "# Here we define the subset (somewhat arbitrary):\n",
    "subset_of_ids = list(key for key in Summaries.keys() if 33000000 <= key < 34000000)\n",
    "\n",
    "# Building our smarter index:\n",
    "for id in sorted(subset_of_ids):\n",
    "    term_set = set(smart_tokenize_and_preprocess(Summaries[id].title))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(smart_tokenize_and_preprocess(Abstracts[id]))\n",
    "    for term in term_set:\n",
    "        smarter_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the `smarter_and_query` function, based on the function `smarter_tokenize_and_preprocess` you defined above and accessing our new index `smarter_index`. You can start from copying the code for `and_query` from the last assignment. For that to work, you'll also have to copy the code for the `and_merge` function from the last assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and_query_1 = neural brain\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41386-021-01010-9>Brain reactivity during aggressive response in women with premenstrual dysphoric disorder treated with a selective progesterone receptor modulator.</a></strong><br>2021. Kaltsouni E, Fisher PM, Dubol M, Hustad S, Lanzenberger R, Frokjaer VG, Wikström J, Comasco E, Sundström-Poromaa I<br>[ID: 33927343]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.dcn.2021.100952>Evidence for dissociable cognitive and neural pathways from poverty versus maltreatment to deficits in emotion regulation.</a></strong><br>2021. Elsayed NM, Rappaport BI, Luby JL, Barch DM<br>[ID: 33857742]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1098/rstb.2020.0138>Political uncertainty moderates neural evaluation of incongruent policy positions.</a></strong><br>2021. Haas IJ, Baker MN, Gonzalez FJ<br>[ID: 33611996]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fnhum.2021.628417>Contrasting Electroencephalography-Derived Entropy and Neural Oscillations With Highly Skilled Meditators.</a></strong><br>2021. Young JH, Arterberry ME, Martin JP<br>[ID: 33994976]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.compbiomed.2021.104428>Automated accurate emotion recognition system using rhythm-specific deep convolutional neural network technique with multi-channel EEG signals.</a></strong><br>2021. Maheshwari D, Ghosh SK, Tripathy RK, Sharma M, Acharya UR<br>[ID: 33984749]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.neuropsychologia.2021.107882>Social brain networks: Resting-state and task-based connectivity in youth with and without epilepsy.</a></strong><br>2021. Morningstar M, French RC, Mattson WI, Englot DJ, Nelson EE<br>[ID: 33964273]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'and_query_2 = neural network\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_AND(list1, list2):\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    result = []\n",
    "    while ptr1 < len(list1) and ptr2 < len(list2):\n",
    "        if list1[ptr1] == list2[ptr2]:\n",
    "            result.append(list1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif list1[ptr1] < list2[ptr2]:\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "    return result\n",
    "\n",
    "def smarter_and_query(query_string):\n",
    "    tokens = smart_tokenize_and_preprocess(query_string)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    if tokens[0] in smarter_index:\n",
    "        result_docs = list(set(smarter_index[tokens[0]]))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    for token in tokens[1:]:\n",
    "        if token in smarter_index:\n",
    "            result_docs = merge_AND(result_docs, list(set(smarter_index[token])))\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "and_query_1 = \"neural brain\"\n",
    "matched_1 = smarter_and_query(and_query_1)\n",
    "and_query_2 = \"neural network\"\n",
    "matched_2 = smarter_and_query(and_query_2)\n",
    "\n",
    "display(\"and_query_1 = neural brain\\n\")\n",
    "for doc_id in matched_1:\n",
    "    display_summary(doc_id)\n",
    "\n",
    "display(\"and_query_2 = neural network\\n\")\n",
    "for doc_id in matched_2:\n",
    "    display_summary(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Run the query \"dance music Billboard\" with the new `smarter_and_query` function from task 1. Does it return paper *33269144*? Explain what our new smarter function specifically contributes to the result (as compared to our previous naive implementations for tokenization and preprocessing)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smarter_and_query(\"dance music Billboard\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "The terms stemmed do not match any documents in the set indexed in smarter_index.\n",
    "The subset of documents used to build the smarter_index might do not contain documents relevant to this query. The AND logic requires all terms to be present in a document. If even one term is missing in all documents, the result will be empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Now we move to a different subject and use our old index again. That is, we **don't** use the smarter functions defined above for tasks 3 to 5!\n",
    "\n",
    "Create a function `tfidf(t,d)` that returns the tf-idf score of term `t` in document `d` by using `tf(t,d)`, `df(t)` and `num_documents` as defined above. To do this, first implement a function `idf(t)` to calculate the inverse document frequency, and then use this function to calculate the full tf-idf. Use the _add-one-smoothing_ version of idf, so we don't run into problems with terms that don't appear in the collection at all. The relevant formulas can be found on the lecture slides. Use tf-idf with plain (non-logarithmic) term frequency, as applied by scoring variant `ntn`. Test your function with the examples shown below. You can use the `log10(n)` function to calculate the base 10 logarithm.\n",
    "\n",
    "Again, use our old (non-smart) index for this task and the tasks below, and **not** the functions defined in tasks 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.139216613177828\n",
      "3.6235686736932524\n",
      "0.7797987182790485\n"
     ]
    }
   ],
   "source": [
    "from math import log10\n",
    "# Your code here:\n",
    "def idf(t):\n",
    "    #inverse document frequency with add-one smoothing\n",
    "    return log10((num_documents + 1) / (df(t) + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    #tf-idif score\n",
    "    return tf(t, d) * idf(t)\n",
    "\n",
    "\n",
    "print(tfidf('children', 33269144))\n",
    "print(tfidf('music', 33269144))\n",
    "print(tfidf('role', 33269144))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Create a function `query_ntn_nnn(query_string)`, which accepts as input a single query string of one or more words, and returns or prints a list of (up to) 10 best matching documents, along with their score. Use _tf-idf_ to calculate document scores based on the query, applying variant `ntn.nnn`, as above (see the formula for the `ntn.nnn` version of scoring on the lecture slides). Use an auxiliary function `score_ntn_nnn` to calculate the score. The results should be shown in descending order by score.\n",
    "\n",
    "You can start by copying your functions `or_merge` and `or_query` from assignment 2, then expand that to rank the results, making use of the `tfidf(t,d)` function you created above.\n",
    "\n",
    "Demonstrate your function by giving it the exemplary query string \"effect of music and dance for young adults\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 34094650|score: 39.74299733266672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.14336/AD.2020.1107>Body adaptation to Dance: A Gerontological Perspective.</a></strong><br>2021. Gronek P, Boraczyński M, Haas AN, Adamczyk J, Pawlaczyk M, Czarny W, Clark CC, Czerniak U, Demuth A, Celka R, Wycichowska P, Gronek J, Król-Zielińska M<br>[ID: 34094650]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 34094650|score: 39.74299733266672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.14336/AD.2020.1107>Body adaptation to Dance: A Gerontological Perspective.</a></strong><br>2021. Gronek P, Boraczyński M, Haas AN, Adamczyk J, Pawlaczyk M, Czarny W, Clark CC, Czerniak U, Demuth A, Celka R, Wycichowska P, Gronek J, Król-Zielińska M<br>[ID: 34094650]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 34094650|score: 39.74299733266672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.14336/AD.2020.1107>Body adaptation to Dance: A Gerontological Perspective.</a></strong><br>2021. Gronek P, Boraczyński M, Haas AN, Adamczyk J, Pawlaczyk M, Czarny W, Clark CC, Czerniak U, Demuth A, Celka R, Wycichowska P, Gronek J, Król-Zielińska M<br>[ID: 34094650]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 24149889|score: 35.296019525665486\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Effects of music interventions on emotional States and running performance.</strong><br>2011. Lane AM, Davis PA, Devonport TJ<br>[ID: 24149889]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 24149889|score: 35.296019525665486\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Effects of music interventions on emotional States and running performance.</strong><br>2011. Lane AM, Davis PA, Devonport TJ<br>[ID: 24149889]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 24149889|score: 35.296019525665486\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Effects of music interventions on emotional States and running performance.</strong><br>2011. Lane AM, Davis PA, Devonport TJ<br>[ID: 24149889]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 29621947|score: 31.47147833791991\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1177/2331216518765379>Comparison of Two Music Training Approaches on Music and Speech Perception in Cochlear Implant Users.</a></strong><br>2018. Fuller CD, Galvin JJ 3rd, Maat B, Başkent D, Free RH<br>[ID: 29621947]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 29621947|score: 31.47147833791991\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1177/2331216518765379>Comparison of Two Music Training Approaches on Music and Speech Perception in Cochlear Implant Users.</a></strong><br>2018. Fuller CD, Galvin JJ 3rd, Maat B, Başkent D, Free RH<br>[ID: 29621947]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 29621947|score: 31.47147833791991\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1177/2331216518765379>Comparison of Two Music Training Approaches on Music and Speech Perception in Cochlear Implant Users.</a></strong><br>2018. Fuller CD, Galvin JJ 3rd, Maat B, Başkent D, Free RH<br>[ID: 29621947]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_OR(list1, list2):\n",
    "    ptr1 = 0 \n",
    "    ptr2 = 0\n",
    "    result = []\n",
    "    while ptr1 < len(list1) and ptr2 < len(list2):\n",
    "        if list1[ptr1] == list2[ptr2]:\n",
    "            result.append(list1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif list1[ptr1] < list2[ptr2]:\n",
    "            result.append(list1[ptr1])\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            result.append(list2[ptr2])\n",
    "            ptr2 += 1\n",
    "    while ptr1 < len(list1):\n",
    "        result.append(list1[ptr1])\n",
    "        ptr1 += 1\n",
    "    while ptr2 < len(list2):\n",
    "        result.append(list2[ptr2])\n",
    "        ptr2 += 1\n",
    "    return result\n",
    "\n",
    "def query_OR(query):\n",
    "    tokens = preprocess(tokenize(query))\n",
    "    result_docs = list(set(inverted_index[tokens[0]]))\n",
    "    for token in tokens:\n",
    "        if token in inverted_index:\n",
    "            result_docs = merge_OR(result_docs, list(set(inverted_index[token])))\n",
    "    return result_docs\n",
    "\n",
    "def score_ntn_nnn(query_tokens, doc_id):\n",
    "    score = sum(    tfidf(t, doc_id) \n",
    "                    for t in query_tokens \n",
    "                    if t in inverted_index)\n",
    "    return score\n",
    "\n",
    "def query_ntn_nnn(query_string) :\n",
    "    query_tokens = preprocess(tokenize(query_string))\n",
    "    result_docs = query_OR(query_string)\n",
    "    scored_docs = [(doc_id, score_ntn_nnn(query_tokens, doc_id)) for doc_id in result_docs ]\n",
    "    scored_docs.sort(key=lambda x:x[1], reverse=True)\n",
    "    return scored_docs[:9 ]\n",
    "\n",
    "# Example query\n",
    "query_string = \"effect of music and dance for young adults\"\n",
    "best_matches = query_ntn_nnn(query_string)\n",
    "for doc_id, score in best_matches:\n",
    "    print(f\"docID: {doc_id}|score: {score}\")\n",
    "    display_summary(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "In this last task, you should create a second version of the query function from Task 4, called `query_ntc_ntc`. This second version should use, as its name suggests, variant `ntc.ntc` instead of `ntn.nnn`, and therefore apply the cosine similarity measure, in addition to applying _tf-idf_. For this, consult the formula for variant `nnc.nnc` on the lecture slides and adopt it to include the _idf_ metric (that is, add the `t` element of `ntc`). (You can drop the square root of |q| in the formula, as indicated on the slides.)\n",
    "\n",
    "As a first step, we can calculate beforehand the length of all document vectors (because they don't depend on the query) for document vectors consisting of _tf-idf_ values. The code below does just that, assuming that you defined the function `tfidf(t,d)` above (don't change this code block, just run it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_length_values = defaultdict(int)\n",
    "\n",
    "for doc_id in Summaries.keys():\n",
    "    l = 0\n",
    "    for t in tf_matrix[doc_id].keys():\n",
    "        l += tfidf(t,doc_id) ** 2\n",
    "    tfidf_length_values[doc_id] = sqrt(l)\n",
    "\n",
    "def tfidf_length(d):\n",
    "    return tfidf_length_values[d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the length of a document vector by calling `tfidf_length(d)`.\n",
    "\n",
    "Based on this, you can now implement `query_ntc_ntc` in the code block below. You should again first define an auxiliary function, called `score_ntc_ntc`. You can start by copy-pasting the code from Task 4.\n",
    "\n",
    "To output the results, use the provided `display_summary` function to make the output a bit more like the results page of a search engine. Lastly, demonstrate your `query_ntc_ntc` function with the same example query as above: \"effect of music and dance for young adults\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 34094650 | score: 0.7552693397230237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.14336/AD.2020.1107>Body adaptation to Dance: A Gerontological Perspective.</a></strong><br>2021. Gronek P, Boraczyński M, Haas AN, Adamczyk J, Pawlaczyk M, Czarny W, Clark CC, Czerniak U, Demuth A, Celka R, Wycichowska P, Gronek J, Król-Zielińska M<br>[ID: 34094650]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 26266024 | score: 0.7196124830915072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.4081/mi.2015.5784>The Change of Music Preferences Following the Onset of a Mental Disorder.</a></strong><br>2015. Gebhardt S, von Georgi R<br>[ID: 26266024]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 29238298 | score: 0.6777432716909281\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fnhum.2017.00572>Enhancement of Pleasure during Spontaneous Dance.</a></strong><br>2017. Bernardi NF, Bellemare-Pepin A, Peretz I<br>[ID: 29238298]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 22530296 | score: 0.6676443039363481\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.2190/om.64.4.c>Beyond words: some uses of music in the funeral setting.</a></strong><br>2011. Caswell G<br>[ID: 22530296]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 24568004 | score: 0.6440040678526759\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1093/jmt/50.3.198>A systematic review on the neural effects of music on emotion regulation: implications for music therapy practice.</a></strong><br>2013. Moore KS<br>[ID: 24568004]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 25725908 | score: 0.632672495962279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/bs.pbr.2014.11.019>Music evolution and neuroscience.</a></strong><br>2015. Snowdon CT, Zimmermann E, Altenmüller E<br>[ID: 25725908]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 24149889 | score: 0.6172513091846612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Effects of music interventions on emotional States and running performance.</strong><br>2011. Lane AM, Davis PA, Devonport TJ<br>[ID: 24149889]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 12748612 | score: 0.6166711989783237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/423221c>Making a song and dance about emotion.</a></strong><br>2003. Marshall RC<br>[ID: 12748612]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 21073970 | score: 0.6037797700838912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.neuroimage.2010.11.017>Congruence of happy and sad emotion in music and faces modifies cortical audiovisual activation.</a></strong><br>2011. Jeong JW, Diwadkar VA, Chugani CD, Sinsoongsud P, Muzik O, Behen ME, Chugani HT, Chugani DC<br>[ID: 21073970]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(query_vector, document_vector):\n",
    "    \"\"\"cosine_similarity function to check for zero magnitude vectors & handle them. \n",
    "If either vector has zero magnitude, the cosine similarity should be set to 0, \n",
    "as there is no overlap in terms.\n",
    "\"\"\"\n",
    "    norm_query = np.linalg.norm(query_vector)\n",
    "    norm_document = np.linalg.norm(document_vector)\n",
    "    # Check for \"zero magnitude\n",
    "    if norm_query == 0 or norm_document == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.dot(query_vector, document_vector) / (norm_query * norm_document)\n",
    "\n",
    "def score_ntc_ntc(query_tokens, doc_id):\n",
    "    all_terms = set(query_tokens) | set(tf_matrix[doc_id].keys())# craft a unified set of terms from both query and document\n",
    "    query_vector = [tfidf(t, doc_id) if t in query_tokens else 0 for t in all_terms]# construct the query vector\n",
    "    document_vector = [tfidf(t, doc_id) for t in all_terms]# create the document vector\n",
    "    return cosine_similarity(query_vector, document_vector)# cosine similarity\n",
    "\n",
    "def query_ntc_ntc(query_string):\n",
    "    query_tokens = preprocess(tokenize(query_string)) \n",
    "    scored_docs = [(doc_id, score_ntc_ntc(query_tokens, doc_id)) for doc_id in Summaries.keys()]# scores for each document\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)# sorting\n",
    "    for doc_id, score in scored_docs[:9 ]:\n",
    "        print(f\"docID: {doc_id} | score: {score}\")\n",
    "        display_summary(doc_id)\n",
    "\n",
    "# Example query:\n",
    "query_ntc_ntc(\"effect of music and dance for young adults\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done **individually**, and that code sharing or copying are **strictly forbidden** and will be punished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
