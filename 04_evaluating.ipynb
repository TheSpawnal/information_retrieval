{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Evaluating Search Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we leave aside the code we developed so far, and look into the more general issue of how to evaluate and compare different search engines. The ultimate test for any Information Retrieval system is how well it is able to satisfy the information needs of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen's Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation will involve the calculation of [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen's_kappa) to quantify the degree to which two human assessors agree or disagree on whether results are considered relevant or not. To calculate Cohen's kappa, we are going to use the [scikit-learn library](http://scikit-learn.org/stable/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/spawn_delta/.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/spawn_delta/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/spawn_delta/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/spawn_delta/.local/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/spawn_delta/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library expects relevance assessments as lists of elements where `1` stands for _relevant_ and `0` stands for _not relevant_, for example like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=[1,0,1,0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list means that the first document was assessed to be relevant, the second to be not relevant, the third to be relevant etc.\n",
    "\n",
    "We need two assessments in order to calculate Cohen's kappa, so let's make another exemplary list that only differs on the last element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=[1,0,1,0,1,0,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now invoke the library as follows to calculate the agreement between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value represents high agreement. We can reach maximal agreement if the two assessments are identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(a1, a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens for a third assessment that differs on three positions with the first one (the three last positions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=[1,0,1,0,1,1,0,1]\n",
    "\n",
    "cohen_kappa_score(a1, a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a smaller but still positive value, because these two assessments still mostly agree. If we make a further example that differs on 6 of the 8 positions, we get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4=[1,0,0,1,0,1,0,1]\n",
    "\n",
    "cohen_kappa_score(a1, a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is now negative, because the two differ on more positions than they agree. The agreement is in fact less than what you would expect to occur just by chance. We get the maximal disagreement if we define a fifth example that disagrees on all positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a5=[0,1,0,1,0,1,0,1]\n",
    "\n",
    "cohen_kappa_score(a1, a5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that the kappa score cannot be calculated if you have only `1`s or only `0`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spawn_delta/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:697: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a6=[1,1,1,1,1,1,1,1]\n",
    "a7=[1,1,1,1,1,1,1,1]\n",
    "\n",
    "cohen_kappa_score(a6, a7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the case of a highly skewed set (either vast majority of agreements on `1` or vast majority of agreements on `0`), the kappa score can be counter-intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1428571428571428"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a8=[1,1,1,1,1,1,0,1]\n",
    "a9=[1,1,1,1,1,1,1,0]\n",
    "\n",
    "cohen_kappa_score(a8, a9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how this function works, we will apply it below for our specific evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Assessments\n",
    "\n",
    "Next, we will define some auxilary code to deal with lists of URLs from search engines and associated relevance assessments. We will encode result lists like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://en.wikipedia.org/wiki/Information_retrieval/',  # 1st result\n",
    "    'http://www.dictionary.com/browse/information',          # 2nd result\n",
    "    'https://nlp.stanford.edu/IR-book/'                      # ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we represent corresponding assessments, as above, as lists of the same size containing relevance values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_assessment = [1, 0, 1]\n",
    "another_assessment = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to nicely display URL lists, with or without related assessments, we define a function called `display_results`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_results(urls, assessment1=None, assessment2=None):\n",
    "    lines = []\n",
    "    lines.append('<table>')\n",
    "    header = '<tr><th>#</th><th>Result URL</th>'\n",
    "    if (assessment1):\n",
    "        header += '<th>Assessment 1</th>'\n",
    "    if (assessment2):\n",
    "        header += '<th>Assessment 2</th>'\n",
    "    header += '</tr>'\n",
    "    lines.append(header)\n",
    "    i = 0\n",
    "    for url in urls:\n",
    "        show_url = url\n",
    "        if (len(url) > 80):\n",
    "            show_url = url[:75] + '...'\n",
    "        line = '<tr><td>{}</td><td><a href=\"{:s}\">{:s}</a></td>'.format(i+1, url, show_url)\n",
    "        if (assessment1):\n",
    "            if (assessment1[i] == 0):\n",
    "                line += '<td><em>Not relevant</em></td>'\n",
    "            else:\n",
    "                line += '<td><strong>Relevant</strong></td>'\n",
    "        if (assessment2):\n",
    "            if (assessment2[i] == 0):\n",
    "                line += '<td><em>Not relevant</em></td>'\n",
    "            else:\n",
    "                line += '<td><strong>Relevant</strong></td>'\n",
    "        line += '</tr>'\n",
    "        lines.append(line)\n",
    "        i = i+1\n",
    "    lines.append('</table>')\n",
    "    display( HTML(''.join(lines)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to display a list of URLs, optionally together with one or two assessment lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just a list of URLs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th></tr><tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Information_retrieval/\">https://en.wikipedia.org/wiki/Information_retrieval/</a></td></tr><tr><td>2</td><td><a href=\"http://www.dictionary.com/browse/information\">http://www.dictionary.com/browse/information</a></td></tr><tr><td>3</td><td><a href=\"https://nlp.stanford.edu/IR-book/\">https://nlp.stanford.edu/IR-book/</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With one assessment:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th><th>Assessment 1</th></tr><tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Information_retrieval/\">https://en.wikipedia.org/wiki/Information_retrieval/</a></td><td><strong>Relevant</strong></td></tr><tr><td>2</td><td><a href=\"http://www.dictionary.com/browse/information\">http://www.dictionary.com/browse/information</a></td><td><em>Not relevant</em></td></tr><tr><td>3</td><td><a href=\"https://nlp.stanford.edu/IR-book/\">https://nlp.stanford.edu/IR-book/</a></td><td><strong>Relevant</strong></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With two assessments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th><th>Assessment 1</th><th>Assessment 2</th></tr><tr><td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Information_retrieval/\">https://en.wikipedia.org/wiki/Information_retrieval/</a></td><td><strong>Relevant</strong></td><td><em>Not relevant</em></td></tr><tr><td>2</td><td><a href=\"http://www.dictionary.com/browse/information\">http://www.dictionary.com/browse/information</a></td><td><em>Not relevant</em></td><td><em>Not relevant</em></td></tr><tr><td>3</td><td><a href=\"https://nlp.stanford.edu/IR-book/\">https://nlp.stanford.edu/IR-book/</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Just a list of URLs:\")\n",
    "display_results(urls)\n",
    "\n",
    "print(\"With one assessment:\")\n",
    "display_results(urls, my_assessment)\n",
    "\n",
    "print(\"With two assessments:\")\n",
    "display_results(urls, my_assessment, another_assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to perform an actual evaluation, which will involve a substantial amount of manual work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** Aldric de Jacquelin #2711498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Think up and formulate a information need (for example in the field of Computer Science or Medicine) for which you think the answer can be found in scientific publications. On page 152 in the book an example of such an information need is shown: \"Information on whether drinking red wine is more effective at reducing the risk of heart attacks than white wine.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**information need: Ozempic treatment efficiency of adult patients with diabetes mellitus to improve glycemic control, in combination with metformin (second-line treatment),and sulfonylurea (third-line treatment);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write down specifically what documents have to look like to satisfy your information need. For example if your information need is about finding an overview of different cancer types, you could state that a document would need to list at least ten types of cancer to satisfy your information need (among other criteria). Write this down as a protocol with rules and examples. For example, such a protocol could state that at least three out of five given criteria have to be fulfilled for a document to be considered relevant for the information need, and then specify the criteria. Or your protocol could have the form of a sequence of rules, where each rule lets you either label the document as relevant or not relevant, or proceed with the next rule. Such rules and criteria can, for example, be about the general topic of the paper, the concepts mentioned in it, the covered relations between concepts, the type of publication (research paper, overview paper, etc.), the number of references, the types of contained diagrams, and so on, depending on your specified information need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** protocol for document relevance assessment:\n",
    "\n",
    "Criteria for Relevance:\n",
    "-Subject Matter: The document must specifically discuss the use of Ozempic (semaglutide) in the treatment of adult diabetes mellitus.\n",
    "-Treatment Combination: It must examine the combination therapy of Ozempic with metformin and/or sulfonylurea.\n",
    "-Outcome Measures: The document should report on specific outcomes related to glycemic control, such as HbA1c levels, fasting plasma glucose, or other relevant biomarkers.\n",
    "-Study Type: The document should be a peer-reviewed research study, such as a randomized controlled trial (RCT), cohort study, or systematic review/meta-analysis focusing on the treatment efficacy.\n",
    "-Recency: The document must be recent, ideally published within the last five years, to ensure the information is up-to-date.\n",
    "-Sample Size and Statistics: The study should have a sufficient sample size for the results to be statistically significant and ideally would report confidence intervals or effect sizes.\n",
    "\n",
    "Protocol Application Sequence:\n",
    "-Rule 1: If the document does not pertain to the treatment of adult diabetes mellitus, discard it as irrelevant.\n",
    "-Rule 2: If the document pertains to diabetes mellitus treatment but does not specifically discuss Ozempic or the combination with metformin and sulfonylurea, discard it as irrelevant.\n",
    "-Rule 3: If the document discusses the treatment but does not provide specific outcome measures for glycemic control, discard it as irrelevant.\n",
    "-Rule 4: If the document is not a peer-reviewed study or is outdated, discard it as irrelevant.\n",
    "-Rule 5: If the document is a peer-reviewed study on the topic with recent data but lacks sufficient sample size or statistical analysis to support the findings, flag it for further consideration but do not consider it fully relevant.\n",
    "A document is considered relevant if it satisfies all the above rules.\n",
    "\n",
    "Example of relevance assessment in this case:\n",
    "A systematic review published 3 years ago analyzing multiple RCTs of Ozempic in combination with metformin and sulfonylurea, reporting HbA1c levels before and after treatment, with a large sample size and clear statistical results, would be considered highly relevant.\n",
    "in opposite, an older narrative review discussing diabetic treatments in general, without specific data on Ozempic combined with metformin and sulfonylurea, or lacking specific glycemic control outcomes, would be deemed irrelevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Formulate a keyword query that represents the information need. For the example on page 152 in the book (see above), the example query \"wine AND red AND white AND heart AND attack AND effective\" is given. (You don't need to use connectors like \"AND\", but if you do, make first sure your chosen search engines below actually support them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** keyword query: (Ozempic OR Semaglutide) AND treatment AND diabetes AND glycemic control AND (mellitus OR metformin OR sulfonylurea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then submit your query to **two** of the following academic search engines:\n",
    "\n",
    "- [Google Scholar](https://scholar.google.com) (all science disciplines)\n",
    "- [Semantic Scholar](https://www.semanticscholar.org) (all science disciplines)\n",
    "- [PubMed Search](https://www.ncbi.nlm.nih.gov/pubmed) (Life Sciences / biomedicine)\n",
    "\n",
    "The right choice of two from the three search engine depends on the topic of your information need. If your information need is in the Life Sciences and biomedicine, it's probably best to include PubMed Search, but otherwise you should pick Google Scholar and Semantic Scholar.\n",
    "\n",
    "Extract a list of the top 10 URLs of the lists of each of the search engines given the query. To be ensure that your results are reproducible, it is advised to use the private mode of your browser. Try to access the resulting publications. For the publications where that is not possible (because of dead links or because the publication is pay-walled even within the VU network), exclude them from the list and add more publications to the end of your list (that is, append results number 11, then 12, etc. to ensure you have two lists of 10 publications each). In order to deal with paywalls, you should try accessing the articles from the VU network, use\n",
    "[UBVU Off-Campus\n",
    "Access](http://www.ub.vu.nl.vu-nl.idm.oclc.org/nl/faciliteiten/toegang-buiten-de-campus/index.aspx), or try to find the respective documents from alternative sources (Google Scholar, for example, is very good at finding free PDFs of articles). If you get fewer than 10 results for one of the search engines, modify the keyword query above to make it more inclusive, and then redo the steps of this task.\n",
    "\n",
    "Store your two lists of URLs in the form of Python lists as introduced above. Then, use the `display_results` function to nicely display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research on PubMed With personal assessments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th><th>Assessment 1</th></tr><tr><td>1</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/31305974/\">https://pubmed.ncbi.nlm.nih.gov/31305974/</a></td><td><strong>Relevant</strong></td></tr><tr><td>2</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/31305971/\">https://pubmed.ncbi.nlm.nih.gov/31305971/</a></td><td><strong>Relevant</strong></td></tr><tr><td>3</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/35194917/\">https://pubmed.ncbi.nlm.nih.gov/35194917/</a></td><td><em>Not relevant</em></td></tr><tr><td>4</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/34706925/\">https://pubmed.ncbi.nlm.nih.gov/34706925/</a></td><td><em>Not relevant</em></td></tr><tr><td>5</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/34305810/\">https://pubmed.ncbi.nlm.nih.gov/34305810/</a></td><td><strong>Relevant</strong></td></tr><tr><td>6</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/36050763/\">https://pubmed.ncbi.nlm.nih.gov/36050763/</a></td><td><strong>Relevant</strong></td></tr><tr><td>7</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/29049653/\">https://pubmed.ncbi.nlm.nih.gov/29049653/</a></td><td><em>Not relevant</em></td></tr><tr><td>8</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/34248838/\">https://pubmed.ncbi.nlm.nih.gov/34248838/</a></td><td><strong>Relevant</strong></td></tr><tr><td>9</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/32987188/\">https://pubmed.ncbi.nlm.nih.gov/32987188/</a></td><td><em>Not relevant</em></td></tr><tr><td>10</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/33964002/\">https://pubmed.ncbi.nlm.nih.gov/33964002/</a></td><td><em>Not relevant</em></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research on Gscholar With personal assessments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th><th>Assessment 1</th></tr><tr><td>1</td><td><a href=\"https://jamanetwork.com/journals/jama/article-abstract/2729339?casa_token=rhDKWigEvHQAAAAA:32qzghsZ7LtTCiQqFjWDHt1CMMC5WdWrL5Ww3TKcLTUfBykIf67p8M1M59IjwaLk5-KgqdMLlGQ\">https://jamanetwork.com/journals/jama/article-abstract/2729339?casa_token=r...</a></td><td><strong>Relevant</strong></td></tr><tr><td>2</td><td><a href=\"https://www.sciencedirect.com/science/article/pii/S1499267118301096\">https://www.sciencedirect.com/science/article/pii/S1499267118301096</a></td><td><strong>Relevant</strong></td></tr><tr><td>3</td><td><a href=\"https://jamanetwork.com/journals/jama/article-abstract/2657376\">https://jamanetwork.com/journals/jama/article-abstract/2657376</a></td><td><strong>Relevant</strong></td></tr><tr><td>4</td><td><a href=\"https://link.springer.com/article/10.1007/s00125-017-4289-0\">https://link.springer.com/article/10.1007/s00125-017-4289-0</a></td><td><strong>Relevant</strong></td></tr><tr><td>5</td><td><a href=\"https://www.thelancet.com/journals/landia/article/PIIS2213-8587(17)30085-2/fulltext\">https://www.thelancet.com/journals/landia/article/PIIS2213-8587(17)30085-2/...</a></td><td><strong>Relevant</strong></td></tr><tr><td>6</td><td><a href=\"https://diabetesjournals.org/care/article/42/12/2262/36240/Efficacy-Safety-and-Tolerability-of-Oral\">https://diabetesjournals.org/care/article/42/12/2262/36240/Efficacy-Safety-...</a></td><td><strong>Relevant</strong></td></tr><tr><td>7</td><td><a href=\"https://diabetesjournals.org/care/article/42/12/2272/36229/Oral-Semaglutide-Versus-Empagliflozin-in-Patients\">https://diabetesjournals.org/care/article/42/12/2272/36229/Oral-Semaglutide...</a></td><td><em>Not relevant</em></td></tr><tr><td>8</td><td><a href=\"https://www.tandfonline.com/doi/full/10.1080/13696998.2019.1614009\">https://www.tandfonline.com/doi/full/10.1080/13696998.2019.1614009</a></td><td><strong>Relevant</strong></td></tr><tr><td>9</td><td><a href=\"https://dom-pubs.onlinelibrary.wiley.com/doi/full/10.1111/dom.13564\">https://dom-pubs.onlinelibrary.wiley.com/doi/full/10.1111/dom.13564</a></td><td><em>Not relevant</em></td></tr><tr><td>10</td><td><a href=\"https://www.thelancet.com/article/S2213-8587(17)30092-X/fulltext\">https://www.thelancet.com/article/S2213-8587(17)30092-X/fulltext</a></td><td><strong>Relevant</strong></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create two of the lists below, depending on your chosen engines:\n",
    "\n",
    "urls_PubMed = [\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/31305974/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/31305971/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/35194917/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/34706925/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/34305810/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/36050763/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/29049653/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/34248838/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/32987188/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/33964002/'\n",
    "]\n",
    "urls_Gscholar = [\n",
    "    'https://jamanetwork.com/journals/jama/article-abstract/2729339?casa_token=rhDKWigEvHQAAAAA:32qzghsZ7LtTCiQqFjWDHt1CMMC5WdWrL5Ww3TKcLTUfBykIf67p8M1M59IjwaLk5-KgqdMLlGQ',\n",
    "    'https://www.sciencedirect.com/science/article/pii/S1499267118301096',\n",
    "    'https://jamanetwork.com/journals/jama/article-abstract/2657376',\n",
    "    'https://link.springer.com/article/10.1007/s00125-017-4289-0',\n",
    "    'https://www.thelancet.com/journals/landia/article/PIIS2213-8587(17)30085-2/fulltext',\n",
    "    'https://diabetesjournals.org/care/article/42/12/2262/36240/Efficacy-Safety-and-Tolerability-of-Oral',\n",
    "    'https://diabetesjournals.org/care/article/42/12/2272/36229/Oral-Semaglutide-Versus-Empagliflozin-in-Patients',\n",
    "    'https://www.tandfonline.com/doi/full/10.1080/13696998.2019.1614009',\n",
    "    'https://dom-pubs.onlinelibrary.wiley.com/doi/full/10.1111/dom.13564',\n",
    "    'https://www.thelancet.com/article/S2213-8587(17)30092-X/fulltext'\n",
    "]\n",
    "# Call display_results here\n",
    "personal_assessment_PubMed =    [1,1,0,0,1,1,0,1,0,0]\n",
    "personal_assessment_Gscholar =  [1,1,1,1,1,1,0,1,0,1] \n",
    "\n",
    "print(\"research on PubMed With personal assessments:\")\n",
    "display_results(urls_PubMed, personal_assessment_PubMed)\n",
    "print(\"research on Gscholar With personal assessments:\")\n",
    "display_results(urls_Gscholar, personal_assessment_Gscholar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Then, find a fellow student who will **independently**\n",
    "assess the results as \"relevant\" or \"not relevant\" using the protocol that you\n",
    "have defined above, and also help (at least) one other student for his/her\n",
    "assessment. Write down their names here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name of the student who assesses my results:** Mohammad Reshadati\n",
    "\n",
    "**Name of the student who I help to assess his/her results:** Mohammad Reshadati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show to the other assessor everything you have written down above for Tasks 1 and 2 (and you might also want to give him/her the PDFs you got for these papers to simplify the process).\n",
    "\n",
    "You as assessors need to stick to the protocol you made in Task 1 and should not discuss with each other, especially when you doubt whether a result is relevant or not. Write down your assessments as lists of relevance values, as introduced above, and make sure they correctly map to the URLs by displaying them together with the `display_results` function.\n",
    "\n",
    "To avoid problems with extreme results, mark in each list at least one paper as 'relevant' and at least one paper as 'not relevant'. That is, if all papers seem relevant, mark the one that seems least relevant 'not relevant', and conversely, if none of the papers seem relevant, mark the one that seems a bit more relevant than the others as 'relevant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research on PubMed With personal assessments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th><th>Assessment 1</th><th>Assessment 2</th></tr><tr><td>1</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/31305974/\">https://pubmed.ncbi.nlm.nih.gov/31305974/</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>2</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/31305971/\">https://pubmed.ncbi.nlm.nih.gov/31305971/</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>3</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/35194917/\">https://pubmed.ncbi.nlm.nih.gov/35194917/</a></td><td><em>Not relevant</em></td><td><em>Not relevant</em></td></tr><tr><td>4</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/34706925/\">https://pubmed.ncbi.nlm.nih.gov/34706925/</a></td><td><em>Not relevant</em></td><td><strong>Relevant</strong></td></tr><tr><td>5</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/34305810/\">https://pubmed.ncbi.nlm.nih.gov/34305810/</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>6</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/36050763/\">https://pubmed.ncbi.nlm.nih.gov/36050763/</a></td><td><strong>Relevant</strong></td><td><em>Not relevant</em></td></tr><tr><td>7</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/29049653/\">https://pubmed.ncbi.nlm.nih.gov/29049653/</a></td><td><em>Not relevant</em></td><td><strong>Relevant</strong></td></tr><tr><td>8</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/34248838/\">https://pubmed.ncbi.nlm.nih.gov/34248838/</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>9</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/32987188/\">https://pubmed.ncbi.nlm.nih.gov/32987188/</a></td><td><em>Not relevant</em></td><td><em>Not relevant</em></td></tr><tr><td>10</td><td><a href=\"https://pubmed.ncbi.nlm.nih.gov/33964002/\">https://pubmed.ncbi.nlm.nih.gov/33964002/</a></td><td><em>Not relevant</em></td><td><em>Not relevant</em></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research on Gscholar With personal assessments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>#</th><th>Result URL</th><th>Assessment 1</th><th>Assessment 2</th></tr><tr><td>1</td><td><a href=\"https://jamanetwork.com/journals/jama/article-abstract/2729339?casa_token=rhDKWigEvHQAAAAA:32qzghsZ7LtTCiQqFjWDHt1CMMC5WdWrL5Ww3TKcLTUfBykIf67p8M1M59IjwaLk5-KgqdMLlGQ\">https://jamanetwork.com/journals/jama/article-abstract/2729339?casa_token=r...</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>2</td><td><a href=\"https://www.sciencedirect.com/science/article/pii/S1499267118301096\">https://www.sciencedirect.com/science/article/pii/S1499267118301096</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>3</td><td><a href=\"https://jamanetwork.com/journals/jama/article-abstract/2657376\">https://jamanetwork.com/journals/jama/article-abstract/2657376</a></td><td><strong>Relevant</strong></td><td><em>Not relevant</em></td></tr><tr><td>4</td><td><a href=\"https://link.springer.com/article/10.1007/s00125-017-4289-0\">https://link.springer.com/article/10.1007/s00125-017-4289-0</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>5</td><td><a href=\"https://www.thelancet.com/journals/landia/article/PIIS2213-8587(17)30085-2/fulltext\">https://www.thelancet.com/journals/landia/article/PIIS2213-8587(17)30085-2/...</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>6</td><td><a href=\"https://diabetesjournals.org/care/article/42/12/2262/36240/Efficacy-Safety-and-Tolerability-of-Oral\">https://diabetesjournals.org/care/article/42/12/2262/36240/Efficacy-Safety-...</a></td><td><strong>Relevant</strong></td><td><em>Not relevant</em></td></tr><tr><td>7</td><td><a href=\"https://diabetesjournals.org/care/article/42/12/2272/36229/Oral-Semaglutide-Versus-Empagliflozin-in-Patients\">https://diabetesjournals.org/care/article/42/12/2272/36229/Oral-Semaglutide...</a></td><td><em>Not relevant</em></td><td><strong>Relevant</strong></td></tr><tr><td>8</td><td><a href=\"https://www.tandfonline.com/doi/full/10.1080/13696998.2019.1614009\">https://www.tandfonline.com/doi/full/10.1080/13696998.2019.1614009</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr><tr><td>9</td><td><a href=\"https://dom-pubs.onlinelibrary.wiley.com/doi/full/10.1111/dom.13564\">https://dom-pubs.onlinelibrary.wiley.com/doi/full/10.1111/dom.13564</a></td><td><em>Not relevant</em></td><td><em>Not relevant</em></td></tr><tr><td>10</td><td><a href=\"https://www.thelancet.com/article/S2213-8587(17)30092-X/fulltext\">https://www.thelancet.com/article/S2213-8587(17)30092-X/fulltext</a></td><td><strong>Relevant</strong></td><td><strong>Relevant</strong></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0 = not relevant; 1 = relevant\n",
    "\n",
    "# You only need to create 4 of the following 6 lists, again depending on which search engines you chose.\n",
    "\n",
    "# Assessment 1 is from you:\n",
    "\n",
    "personal_assessment_PubMed =    [1,1,0,0,1,1,0,1,0,0]\n",
    "personal_assessment_Gscholar =  [1,1,1,1,1,1,0,1,0,1] \n",
    "# Assessment 2 is from another assessor (don't show him/her your own assessment!):\n",
    "\n",
    "assessment2_PubMed =    [1,1,0,1,1,0,1,1,0,0]\n",
    "assessment2_Gscholar =  [1,1,0,1,1,0,1,1,0,1]\n",
    "\n",
    "# Call display_results here\n",
    "\n",
    "urls_PubMed = [\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/31305974/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/31305971/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/35194917/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/34706925/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/34305810/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/36050763/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/29049653/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/34248838/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/32987188/',\n",
    "    'https://pubmed.ncbi.nlm.nih.gov/33964002/'\n",
    "]\n",
    "urls_Gscholar = [\n",
    "    'https://jamanetwork.com/journals/jama/article-abstract/2729339?casa_token=rhDKWigEvHQAAAAA:32qzghsZ7LtTCiQqFjWDHt1CMMC5WdWrL5Ww3TKcLTUfBykIf67p8M1M59IjwaLk5-KgqdMLlGQ',\n",
    "    'https://www.sciencedirect.com/science/article/pii/S1499267118301096',\n",
    "    'https://jamanetwork.com/journals/jama/article-abstract/2657376',\n",
    "    'https://link.springer.com/article/10.1007/s00125-017-4289-0',\n",
    "    'https://www.thelancet.com/journals/landia/article/PIIS2213-8587(17)30085-2/fulltext',\n",
    "    'https://diabetesjournals.org/care/article/42/12/2262/36240/Efficacy-Safety-and-Tolerability-of-Oral',\n",
    "    'https://diabetesjournals.org/care/article/42/12/2272/36229/Oral-Semaglutide-Versus-Empagliflozin-in-Patients',\n",
    "    'https://www.tandfonline.com/doi/full/10.1080/13696998.2019.1614009',\n",
    "    'https://dom-pubs.onlinelibrary.wiley.com/doi/full/10.1111/dom.13564',\n",
    "    'https://www.thelancet.com/article/S2213-8587(17)30092-X/fulltext'\n",
    "]\n",
    "# Call display_results here\n",
    "print(\"research on PubMed With personal assessments:\")\n",
    "display_results(urls_PubMed, personal_assessment_PubMed, assessment2_PubMed)\n",
    "print(\"research on Gscholar With personal assessments:\")\n",
    "display_results(urls_Gscholar, personal_assessment_Gscholar, assessment2_Gscholar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Compute Cohen's kappa to quantify how much the two assessors agreed. Use the function `cohen_kappa_score` demonstrated above to calculate two times the inter-annotator agreement (once for each of the two search engines), and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa for Google Scholar: 0.21052631578947367\n",
      "Kappa for PubMed: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Add your code here:\n",
    "\n",
    "kappa_google = cohen_kappa_score(personal_assessment_Gscholar,assessment2_Gscholar)\n",
    "kappa_pubmed = cohen_kappa_score(personal_assessment_PubMed,assessment2_PubMed)\n",
    "\n",
    "print(\"Kappa for Google Scholar:\", kappa_google)\n",
    "print(\"Kappa for PubMed:\", kappa_pubmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain whether the agreement can be considered high or not, based on the interpretation table on [this Wikipedia page](https://en.wikipedia.org/wiki/Fleiss'_kappa#Interpretation) (this Wikipedia page is about a different type of kappa but the interpretation table can also be used for Cohen's kappa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer:\n",
    "#Kappa for Google Scholar ~ 0.21: considered as slight to fair agreement.\n",
    "#Kappa for PubMed = 0.4 : falls into the moderate agreement category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer:\n",
    "\n",
    "Kappa for Google Scholar ~ 0.21: considered as slight to fair agreement.\n",
    "\n",
    "Kappa for PubMed = 0.4 : falls into the moderate agreement category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Define a function called `precision_at_n` that calculates Precision@n as described in the lecture slides, which takes as input an assessment list and a value for _n_ and returns the respective Precision@n value. Run this function to calculate Precision@10 (that is, n=10) on all four assessments (two assessors and two search engines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 for personal assessment on PubMed: 0.5\n",
      "Precision@10 for external assessment2 on PubMed: 0.6\n",
      "Precision@10 for personal assessment on Google Scholar: 0.8\n",
      "Precision@10 for external assessment2 on Google Scholar: 0.7\n"
     ]
    }
   ],
   "source": [
    "def precision_at_n(assessment_x, n):\n",
    "    relevant_files = sum(assessment_x[:n])\n",
    "    precision_n = relevant_files / n\n",
    "    return precision_n\n",
    "\n",
    "precision_at_10_personal_PubMed = precision_at_n(personal_assessment_PubMed, 10)\n",
    "precision_at_10_assessment2_PubMed = precision_at_n(assessment2_PubMed, 10)\n",
    "precision_at_10_personal_Gscholar = precision_at_n(personal_assessment_Gscholar, 10)\n",
    "precision_at_10_assessment2_Gscholar = precision_at_n(assessment2_Gscholar, 10)\n",
    "print(\"Precision@10 for personal assessment on PubMed:\", precision_at_10_personal_PubMed)\n",
    "print(\"Precision@10 for external assessment2 on PubMed:\", precision_at_10_assessment2_PubMed)\n",
    "print(\"Precision@10 for personal assessment on Google Scholar:\", precision_at_10_personal_Gscholar)\n",
    "print(\"Precision@10 for external assessment2 on Google Scholar:\", precision_at_10_assessment2_Gscholar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what these specific Precision@10 results tell us (or don't tell us) about the quality of the two search engines for your particular information need. You can also refer to the results of Task 4 if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal assessment on PubMed =0.5: Half of the top 10 results were relevant to the specified information need. This suggests moderate relevance of the search results.\n",
    "\n",
    "External assessment on PubMed=0.6: A slightly higher relevance than the personal assessment, indicating that 60% of the top 10 results were relevant.\n",
    "\n",
    "Personal assessment on Google Scholar = 0.8: A high proportion (80%) of the top10 results were relevant, indicating that Google Scholar provided more relevant results for the query/information need.\n",
    "\n",
    "External Assessment on Google Scholar = 0.7: This is lower than the personal assessment still suggests a high relevance of the search results.\n",
    "\n",
    "Precision@10 results imply that for this specific information need, Google Scholar returned a higher number of relevant documents in its top 10 results compared to PubMed, according to the assessor's and likewise an external evaluator's assessments. \n",
    "However, it doesn't account for the total number of relevant documents in the entire dataset (recall), nor does it consider the rank of the relevant documents beyond the top 10 (which could be captured by average precision or discounted cumulative gain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done **individually**, and that code sharing or copying are **strictly forbidden** and will be punished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
