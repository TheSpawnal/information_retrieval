{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Building a Simple Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will build a simple search index, which we will use later for Boolean retrieval. The assignment tasks are again at the bottom of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Summaries_file = 'data/emotion_Summaries.pkl.bz2'\n",
    "Abstracts_file = 'data/emotion_Abstracts.pkl.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle, bz2\n",
    "from collections import namedtuple\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )\n",
    "    \n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what the data looks like for an example of a paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper(title='Visual and auditory brain areas share a representational structure that supports emotion perception.', authors=['Sievers B', 'Parkinson C', 'Kohler PJ', 'Hughes JM', 'Fogelson SV', 'Wheatley T'], year=2021, doi='10.1016/j.cub.2021.09.043')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summaries[34644547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emotionally expressive music and dance occur together across the world. This may be because features shared across the senses are represented the same way even in different sensory brain areas, putting music and movement in directly comparable terms. These shared representations may arise from a general need to identify environmentally relevant combinations of sensory features, particularly those that communicate emotion. To test the hypothesis that visual and auditory brain areas share a representational structure, we created music and animation stimuli with crossmodally matched features expressing a range of emotions. Participants confirmed that each emotion corresponded to a set of features shared across music and movement. A subset of participants viewed both music and animation during brain scanning, revealing that representations in auditory and visual brain areas were similar to one another. This shared representation captured not only simple stimulus features but also combinations of features associated with emotion judgments. The posterior superior temporal cortex represented both music and movement using this same structure, suggesting supramodal abstraction of sensory content. Further exploratory analysis revealed that early visual cortex used this shared representational structure even when stimuli were presented auditorily. We propose that crossmodally shared representations support mutually reinforcing dynamics across auditory and visual brain areas, facilitating crossmodal comparison. These shared representations may help explain why emotions are so readily perceived and why some dynamic emotional expressions can generalize across cultural contexts.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstracts[34644547]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define some utility functions that allow us to tokenize a string into terms, perform linguistic preprocessing on a list of terms, as well as a function to display information about a paper in a nice way. Note that these tokenization and preprocessing functions are rather naive. We will improve them in a later assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emotionally', 'expressive', 'music', 'and', 'dance', 'occur', 'together', 'across', 'the', 'world.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on a list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "print(preprocess(tokenize(\"Emotionally expressive music and dance occur together across the world.\")))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.cub.2021.09.043>Visual and auditory brain areas share a representational structure that supports emotion perception.</a></strong><br>2021. Sievers B, Parkinson C, Kohler PJ, Hughes JM, Fogelson SV, Wheatley T<br><small><strong>Abstract:</strong> <em>Emotionally expressive music and dance occur together across the world. This may be because features shared across the senses are represented the same way even in different sensory brain areas, putting music and movement in directly comparable terms. These shared representations may arise from a general need to identify environmentally relevant combinations of sensory features, particularly those that communicate emotion. To test the hypothesis that visual and auditory brain areas share a representational structure, we created music and animation stimuli with crossmodally matched features expressing a range of emotions. Participants confirmed that each emotion corresponded to a set of features shared across music and movement. A subset of participants viewed both music and animation during brain scanning, revealing that representations in auditory and visual brain areas were similar to one another. This shared representation captured not only simple stimulus features but also combinations of features associated with emotion judgments. The posterior superior temporal cortex represented both music and movement using this same structure, suggesting supramodal abstraction of sensory content. Further exploratory analysis revealed that early visual cortex used this shared representational structure even when stimuli were presented auditorily. We propose that crossmodally shared representations support mutually reinforcing dynamics across auditory and visual brain areas, facilitating crossmodal comparison. These shared representations may help explain why emotions are so readily perceived and why some dynamic emotional expressions can generalize across cultural contexts.</em></small><br>[ID: 34644547]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def display_summary( id, show_abstract=False, show_id=True, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long author lists, and adds a link to the paper's DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[id]\n",
    "    lines = []\n",
    "    title = s.title\n",
    "    if s.doi != '':\n",
    "        title = '<a href=http://dx.doi.org/{:s}>{:s}</a>'.format(s.doi, title)\n",
    "    title = '<strong>' + title + '</strong>'\n",
    "    lines.append(title)\n",
    "    authors = ', '.join( s.authors[:20] ) + ('' if len(s.authors) <= 20 else ', ...')\n",
    "    lines.append(str(s.year) + '. ' + authors)\n",
    "    if (show_abstract):\n",
    "        lines.append('<small><strong>Abstract:</strong> <em>{:s}</em></small>'.format(Abstracts[id]))\n",
    "    if (show_id):\n",
    "        lines.append('[ID: {:d}]'.format(id))\n",
    "    if (extra_text != ''):\n",
    "         lines.append(extra_text)\n",
    "    display( HTML('<br>'.join(lines)) )\n",
    "\n",
    "display_summary(34644547, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our first index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an _inverted index_ based on the words in the titles and abstracts of the papers in our dataset. We will implement our inverted index as a Python dictionary with term strings as keys and posting lists (implemented as Python lists) as values. We include all the tokens we can find in the title and (if available) in the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "# This can take a few seconds:\n",
    "for id in sorted(Summaries.keys()):\n",
    "    term_set = set(preprocess(tokenize(Summaries[id].title)))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(preprocess(tokenize(Abstracts[id])))\n",
    "    for term in term_set:\n",
    "        inverted_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in the index for the example term 'music':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[616164, 1858435, 2631969, 2728537, 3835976, 6153964, 7266193, 7510804, 7870518, 8259767, 8541967, 9076708, 9567714, 9602525, 9606949, 9818509, 9949709, 10053235, 10138592, 10260225, 10586571, 11129375, 11161359, 11407962, 11573015, 11815880, 11915127, 11990972, 12133576, 12296486, 12575587, 12678572, 12761468, 12795548, 12956543, 14565905, 14589533, 14681113, 15053726, 15182241, 15315187, 15327344, 15699060, 15709930, 15913388, 15993964, 16078183, 16086981, 16271503, 16384553, 16447368, 16458860, 16597758, 16597772, 16597776, 16597795, 16671835, 16761812, 16828911, 16935277, 16959817, 16970965, 17290372, 17295965, 17323817, 17343712, 17395609, 17416488, 17466401, 17516815, 17554674, 17695356, 17965981, 17997625, 18039047, 18189577, 18392933, 18501389, 18505596, 18558143, 18588701, 18710596, 18729581, 18753014, 18755225, 18824047, 18826699, 18985111, 18988943, 19102595, 19227107, 19253088, 19256729, 19368861, 19391047, 19526057, 19588251, 19592998, 19594648, 19673788, 19673808, 19673809, 19673812, 19673837, 19822162, 19884142, 19955723, 20005297, 20025907, 20153297, 20300041, 20300580, 20307678, 20338217, 20442037, 20515223, 20524563, 20551355, 20633975, 20663569, 20831571, 20887648, 21071094, 21071617, 21073970, 21097307, 21141770, 21146164, 21179549, 21180883, 21181251, 21217764, 21252549, 21264050, 21309873, 21385617, 21477909, 21526586, 21527791, 21530980, 21544264, 21547762, 21559468, 21626088, 21648329, 21678187, 21683947, 21696717, 21704661, 21707144, 21714738, 21714965, 21716581, 21723130, 21747773, 21756454, 21756455, 21763342, 21767048, 21778809, 21859191, 21859207, 21895104, 21902567, 21942696, 22022516, 22025908, 22110619, 22144968, 22232614, 22242169, 22248246, 22255729, 22292000, 22303405, 22362607, 22405960, 22419117, 22431970, 22505222, 22506300, 22524374, 22530296, 22534936, 22579645, 22642351, 22721000, 22811384, 22811391, 22835930, 22842396, 22956988, 23057507, 23112175, 23125108, 23145287, 23220447, 23227458, 23248314, 23269918, 23281860, 23285951, 23350300, 23367102, 23386738, 23391882, 23418992, 23422457, 23440757, 23477505, 23516087, 23527508, 23576998, 23605956, 23630507, 23641223, 23671106, 23684870, 23732338, 23750144, 23754373, 23769678, 23785342, 23810975, 23882233, 23908642, 23914179, 23944764, 23964250, 23964255, 23970875, 23974947, 24046758, 24062634, 24101908, 24137141, 24149889, 24151530, 24175181, 24215647, 24236079, 24290272, 24298171, 24319434, 24348344, 24380823, 24381565, 24389368, 24399950, 24427146, 24432008, 24526569, 24552785, 24568004, 24581119, 24603216, 24604603, 24672492, 24729748, 24795677, 24808096, 24822035, 24829557, 24847111, 24871301, 24891698, 24910621, 24923421, 24939575, 24957406, 25014925, 25023618, 25026154, 25050430, 25066878, 25071428, 25072162, 25076869, 25101043, 25144200, 25167363, 25224999, 25251882, 25291405, 25320700, 25324094, 25330315, 25339880, 25389411, 25440305, 25459139, 25461917, 25490753, 25505879, 25534332, 25551392, 25562621, 25565997, 25566122, 25620935, 25635400, 25646521, 25674060, 25674066, 25684284, 25684286, 25691513, 25725908, 25725918, 25726292, 25740903, 25741232, 25741278, 25741295, 25773627, 25773635, 25773636, 25773637, 25773639, 25773641, 25813790, 25814950, 25814961, 25837268, 25899831, 25932010, 25954217, 25994970, 26052277, 26080754, 26082702, 26083383, 26124532, 26203921, 26217252, 26226563, 26248474, 26257162, 26257625, 26266024, 26300835, 26301531, 26302716, 26304229, 26316123, 26317976, 26347603, 26354957, 26355193, 26379529, 26379583, 26392676, 26441718, 26455803, 26504353, 26520146, 26528171, 26545104, 26553987, 26578990, 26578992, 26579029, 26605497, 26617511, 26642050, 26656189, 26681623, 26696818, 26725925, 26778996, 26787218, 26834675, 26863420, 26913488, 26923742, 26925009, 26936595, 26970942, 26973584, 27019776, 27065893, 27084302, 27187232, 27217116, 27221253, 27252770, 27284390, 27284693, 27300268, 27311295, 27318599, 27362475, 27375537, 27394152, 27415015, 27445674, 27445752, 27478480, 27489560, 27526666, 27530829, 27534322, 27536232, 27611092, 27678301, 27689591, 27695424, 27746579, 27747819, 27770078, 27790111, 27799544, 27848281, 27867061, 28040801, 28069444, 28152081, 28159618, 28228741, 28282400, 28349620, 28367404, 28371645, 28373851, 28377740, 28387335, 28421007, 28421015, 28421017, 28424637, 28452679, 28460035, 28461128, 28491045, 28496420, 28596746, 28658285, 28717902, 28720526, 28720776, 28764896, 28781419, 28790442, 28846006, 28855881, 28894831, 28958161, 28986700, 29025134, 29028939, 29033810, 29060499, 29060614, 29093672, 29094761, 29163276, 29163286, 29225563, 29235615, 29238298, 29239655, 29249997, 29255434, 29290641, 29311870, 29311874, 29312024, 29342779, 29354080, 29367844, 29380139, 29385142, 29396616, 29411997, 29447561, 29472873, 29478708, 29541041, 29551984, 29580190, 29621947, 29696045, 29706916, 29714026, 29714918, 29715736, 29727009, 29732575, 29741242, 29745546, 29749625, 29772960, 29779745, 29797585, 29853841, 29857757, 29867629, 29867690, 29922139, 29926237, 29933380, 29936120, 29953870, 30043178, 30073859, 30135649, 30145207, 30181063, 30195538, 30210316, 30272211, 30321036, 30321389, 30334227, 30369705, 30374854, 30391223, 30409082, 30440815, 30445068, 30459665, 30460234, 30473677, 30475035, 30477866, 30479734, 30483173, 30500394, 30506118, 30518885, 30558474, 30592696, 30629944, 30632918, 30664457, 30687158, 30816742, 30832292, 30852910, 30883569, 30890020, 30890993, 30894829, 30910074, 30913416, 30936827, 30956655, 30956720, 30990819, 30999560, 31024405, 31026474, 31050205, 31059486, 31137662, 31145745, 31153307, 31178787, 31202155, 31219478, 31263113, 31317962, 31372689, 31391062, 31402880, 31404816, 31482571, 31512332, 31518379, 31532781, 31549425, 31551686, 31551857, 31555172, 31615998, 31619943, 31619969, 31638882, 31642389, 31647961, 31664132, 31708368, 31725733, 31742643, 31776972, 31805815, 31820677, 31877974, 31901418, 31906793, 31907316, 31926279, 31967195, 31977888, 31984963, 31993668, 32023136, 32050086, 32065140, 32096377, 32116613, 32123973, 32132904, 32176126, 32268523, 32277251, 32302600, 32354094, 32358988, 32379657, 32422128, 32434622, 32442777, 32541806, 32547455, 32558624, 32567335, 32581943, 32611175, 32612553, 32639926, 32640373, 32670037, 32670038, 32718172, 32832103, 32841260, 32848961, 32898679, 32903573, 32908227, 32926234, 32928851, 32982862, 33005139, 33013602, 33015164, 33018116, 33018685, 33019236, 33059197, 33074236, 33090055, 33154663, 33189951, 33192408, 33201423, 33206664, 33224021, 33232856, 33252967, 33269144, 33281651, 33301827, 33367590, 33370739, 33391081, 33422540, 33439887, 33442987, 33474716, 33488453, 33511447, 33568976, 33584402, 33613395, 33733192, 33798055, 33808786, 33808989, 33828508, 33828512, 33828693, 33868120, 33868125, 33889107, 33897346, 33945156, 33980876, 33981274, 33996929, 34017286, 34025335, 34063693, 34093328, 34098026, 34135825, 34145586, 34217125, 34248787, 34280999, 34297831, 34300436, 34300666, 34305560, 34305714, 34309409, 34329821, 34354631, 34393944, 34421523, 34421756, 34453176, 34456670, 34478815, 34508286, 34512453, 34527043, 34527046, 34557268, 34588026, 34588054, 34594267, 34611377, 34615904, 34644547]\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index['music'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that's a lot of research related to music and emotion! We can now use this inverted index to answer simple one-word queries, for example to show all papers that contain the word 'dance':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_word = 'dance'\n",
    "for i in inverted_index[query_word]:\n",
    "    display_summary(i, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** Aldric de Jacquelin #2711498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement the function `merge_and` outlined below. This function takes two posting lists from the index that can be assumed to be sorted already (e.g. [3,5,8,10] and [5,7,8,12]), and it should return the result of the merging of the two lists with AND. The resulting list should therefore include all the elements that appear in both lists. As explained on the slides, this operation should take advantage of the input lists being sorted already, should not perform any additional sorting operation, and should go through each of the input lists just once. Then, test your function with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Best Case Time Complexity:      O(min(n, m))\n",
    "Worst Case Time Complexity:     O(n + m)\n",
    "Space Complexity:               O(min(n, m))\n",
    "'''\n",
    "def merge_AND(list1, list2):\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    result = []\n",
    "    while ptr1 < len(list1) and ptr2 <len(list2):\n",
    "        if list1[ptr1] == list2[ptr2]:\n",
    "            result.append(list1[ptr1])\n",
    "            ptr1 +=1\n",
    "            ptr2 +=1\n",
    "        elif list1[ptr1] < list2[ptr2]:\n",
    "            ptr1 +=1\n",
    "        else:\n",
    "            ptr2 +=1\n",
    "    return result\n",
    "\n",
    "list_a = [3,5,8,10]\n",
    "list_b = [5,7,8,12]\n",
    "\n",
    "display(merge_AND(list_a, list_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Similarly as above, implement the function `merge_or` outlined below that executes an OR merging of the lists. The resulting list should therefore include all the elements that appear in at least one of the lists. Again, this operation should take advantage of the input lists being sorted already, should not perform any additional sorting operation, and should go through each of the input lists just once. Elements that appear in both input list should only appear once in the output list. Test your function again with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7, 8, 10, 12]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Best Case Time Complexity:      O(n + m)\n",
    "Space Complexity:               O(n + m)\n",
    "'''\n",
    "def merge_OR(list1, list2):\n",
    "    ptr1 = 0 \n",
    "    ptr2 = 0\n",
    "    result = []\n",
    "    while ptr1 < len(list1) and ptr2 < len(list2):\n",
    "        if list1[ptr1] == list2[ptr2]:\n",
    "            result.append(list1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif list1[ptr1] < list2[ptr2]:\n",
    "            result.append(list1[ptr1])\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            result.append(list2[ptr2])\n",
    "            ptr2 += 1\n",
    "    while ptr1 < len(list1):\n",
    "        result.append(list1[ptr1])\n",
    "        ptr1 += 1\n",
    "    while ptr2 < len(list2):\n",
    "        result.append(list2[ptr2])\n",
    "        ptr2 += 1\n",
    "    return result\n",
    "\n",
    "list_a = [3, 5, 8, 10]\n",
    "list_b = [5, 7, 8, 12]\n",
    "\n",
    "display(merge_OR(list_a, list_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Construct a function called `query_and` that takes as input a single string, consisting of one or more words, and returns as function value a list of matching documents. `query_and`, as its name suggests, should require that all query terms are present in the documents of the result list.\n",
    "\n",
    "For that, access the variable `inverted_index` from above and use the method `merge_and` that you defined. Also use the `tokenize` and `preprocess` functions we defined above to tokenize and preprocess your query.\n",
    "\n",
    "Again demonstrate the working of your function with an example (choose one that leads to fewer than 100 hits to not overblow this notebook file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/spawn_delta/retrieval/02_building.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result_docs\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m and_query_1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdance music Billboard\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m matched_1 \u001b[39m=\u001b[39m query_AND(and_query_1)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m and_query_2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mneural network\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m matched_2 \u001b[39m=\u001b[39m query_AND(and_query_2)\n",
      "\u001b[1;32m/home/spawn_delta/retrieval/02_building.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery_AND\u001b[39m(query):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     tokens \u001b[39m=\u001b[39m preprocess(tokenize(query))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokens[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m inverted_index:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/spawn_delta/retrieval/02_building.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         result_docs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(inverted_index[tokens[\u001b[39m0\u001b[39m]]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "def query_AND(query):\n",
    "\n",
    "    tokens = preprocess(tokenize(query))\n",
    "    if tokens[0] in inverted_index:\n",
    "        result_docs = list(set(inverted_index[tokens[0]]))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    for token in tokens[1:]:\n",
    "        if token in inverted_index:\n",
    "            result_docs = merge_AND(result_docs, list(set(inverted_index[token])))\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "and_query_1 = \"neural brain\"\n",
    "matched_1 = query_AND(and_query_1)\n",
    "and_query_2 = \"neural network\"\n",
    "matched_2 = query_AND(and_query_2)\n",
    "display(\"and_query_1 = neural brain\\n\")\n",
    "for doc_id in matched_1:\n",
    "    display_summary(doc_id, show_abstract=True)\n",
    "display(\"and_query_2 = neural network\\n\")\n",
    "for doc_id in matched_2:\n",
    "    display_summary(doc_id, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Construct another function called `query_or` that works in the same way as `query_and` you just implemented, but returns as function value the documents that contain _at least one_ of the words in the query, using the `merge_or` function you defined.\n",
    "\n",
    "Demonstrate the working of this function also with an example (again, choose one that leads to fewer than 100 hits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1017/S1041610218000170>Comparison of structural connectivity in Parkinson's disease with depressive symptoms versus non-depressed: a diffusion MRI connectometry study.</a></strong><br>2019. Ansari M, Adib Moradi S, Ghazi Sherbaf F, Hedayatnia A, Aarabi MH<br><small><strong>Abstract:</strong> <em>ABSTRACTObjective:Research on psychobiological markers of Parkinson's disease (PD) remains a hot topic. Non-motor symptoms such as depression and REM sleep behavior disorder (RBD) each attribute to a particular neurodegenerative cluster in PD, and might enlighten the way for early prediction/detection of PD. The neuropathology of mood disturbances remains unclear. In fact, a few studies have investigated depression using diffusion magnetic resonance imaging (diffusion MRI).</em></small><br>[ID: 29560834]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Synthetic vision and emotion calculation in intelligent virtual human modeling.</strong><br>2007. Zhao Y, Kang J, Wright DK<br><small><strong>Abstract:</strong> <em>The virtual human technique can already provide vivid and believable human behaviour in more and more scenarios. Virtual humans are expected to replace real humans in hazardous situations to undertake tests and feed back valuable information. This paper will introduce a virtual human with a novel collision-based synthetic vision, short-term memory model and a capability to implement emotion calculation and decision making. The virtual character based on this model can 'see' what is in its field of view (FOV) and remember those objects. After that, a group of affective computing equations have been introduced. These equations have been implemented into a proposed emotion calculation process to enlighten emotion for virtual intelligent humans.</em></small><br>[ID: 17487108]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1037/emo0000028>Assessing the Impact of Anger State on the Three Attentional Networks with the ANT-I.</a></strong><br>2015. Techer F, Jallais C, Fort A, Corson Y<br><small><strong>Abstract:</strong> <em>Anger is a negative and highly aroused emotion. Previous research has revealed that a high level of arousal can induce the participant in a physical preparation and self-awareness. The aim of this research was to study the influence of anger on the attentional network using the Attention Network Test-Interactions (ANT-I). This test has been developed in order to assess 3 attentional networks: alerting, orienting, and executive control. Here, participants were induced in anger using the autobiographic recall procedure or in a neutral mood before the realization of the ANT-I. As expected, the results showed a better alerting score for the angry group. The possible origin of this alerting gain related to the high level of arousal is discussed. The results obtained should enlighten the interaction between emotion and the functioning of the attentional system. They also may be relevant for applied fields related to anger.</em></small><br>[ID: 25286073]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fnhum.2017.00516>The Role of Motivation in Cognitive Reappraisal for Depressed Patients.</a></strong><br>2017. Wang X, Zhou X, Dai Q, Ji B, Feng Z<br><small><strong>Abstract:</strong> <em> Background: People engage in emotion regulation in service of motive goals (typically, to approach a desired emotional goal or avoid an undesired emotional goal). However, how motives (goals) in emotion regulation operate to shape the regulation of emotion is rarely known. Furthermore, the modulatory role of motivation in the impaired reappraisal capacity and neural abnormalities typical of depressed patients is not clear. Our hypothesis was that (1) approach and avoidance motivation may modulate emotion regulation and the underlying neural substrates; (2) approach/avoidance motivation may modulate emotion regulation neural abnormalities in depressed patients. Methods: Twelve drug-free depressed patients and fifteen matched healthy controls reappraised emotional pictures with approach/avoidant strategies and self-rated their emotional intensities during fMRI scans. Approach/avoidance motivation was measured using Behavioral Inhibition System and Behavioral Activation System (BIS/BAS) Scale. We conducted whole-brain analyses and correlation analyses of regions of interest to identify alterations in regulatory prefrontal-amygdala circuits which were modulated by motivation. Results: Depressed patients had a higher level of BIS and lower levels of BAS-reward responsiveness and BAS-drive. BIS scores were positively correlated with depressive severity. We found the main effect of motivation as well as the interactive effect of motivation and group on the neural correlates of emotion regulation. Specifically, hypoactivation of IFG underlying the group differences in the motivation-related neural correlates during reappraisal may be partially explained by the interaction between group and reappraisal. Consistent with our prediction, dlPFC and vmPFC was differentially between groups which were modulated by motivation. Specifically, the avoidance motivation of depressed patients could predict the right dlPFC activation during decreasing positive emotion, while the approach motivation of normal individuals could predict the right vmPFC activation during decreasing negative emotion. Notably, striatal regions were observed when examining the neural substrates underlying the main effect of motivation (lentiform nucleus) and the interactive effect between motivation and group (midbrain). Conclusions: Our findings highlight the modulatory role of approach and avoidance motivation in cognitive reappraisal, which is dysfunctional in depressed patients. The results could enlighten the CBT directed at modifying the motivation deficits in cognitive regulation of emotion. </em></small><br>[ID: 29163097]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41538-020-00071-2>The COVID-19 may help enlightening how emotional food is.</a></strong><br>2020. Coppin G<br><small><strong>Abstract:</strong> <em>Olfactory and gustatory stimuli can elicit potent emotional responses and are essential in food perception. Yet, main theories of emotion often under-represent them, and our understanding of affective phenomena relies mostly on experimental studies conducted on visual and auditory stimuli. Although evidence is still accumulating today, recent findings suggest that the COVID-19 is associated with a loss in olfaction and/or taste. Here, I discuss how this unprecedented and uncommon spread of the loss of olfaction and/or taste worldwide may enlighten how emotional both these senses are and how much they influence food perception.</em></small><br>[ID: 32821852]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fvets.2021.543463>The End of the Partnership With a Guide Dog: Emotional Responses, Effects on Quality of Life and Relationships With Subsequent Dogs.</a></strong><br>2021. Lloyd J, Budge C, La Grow S, Stafford K<br><small><strong>Abstract:</strong> <em>Guide dogs are mobility aids that facilitate independent travel of people who are blind or visually impaired. Additional benefits imparted to the guide dog handler include companionship, and increased: social-function, self-esteem and confidence. Some evidence shows that the end of the guide dog partnership can result in reduced mobility, and may have profound psychosocial effects on the handler due to feelings of bereavement and loss of self-esteem. However, this evidence is limited. This study examined the experiences and feelings of 36 people across New Zealand, who experienced the ending of at least one partnership with a guide dog (77 pairings), to explore issues arising at the end of the partnership and how this may impact on relationships with subsequent dogs. Results indicate that the majority of handlers experienced a reduction in their quality of life due to a decrease in independent mobility followed by the loss of a friend and companion, curtailment of social interactions, and loss of self-esteem/confidence. The end of the partnership affected people in different ways. Most handlers \"accepted\" the partnership had ended, but some felt guilty or angry with the guide dog school. Most applied for another dog immediately, as the need for mobility was high, while others preferred to wait and a smaller number did not reapply. Feelings at this time also affected the handlers' relationships with subsequent guide dogs, with over a quarter expressing a negative effect. Retiring a guide dog (for whatever reason) is not only difficult for the handler, but also for the handler's family, friends, co-workers, and doubtlessly, the dog. The majority of handlers expressed feelings of extreme grief when the partnership ended, whether it was successful or not. Feelings of extreme grief were more common for first than subsequent dogs. The depth of emotion was compared to losing a family member or other loved one, which has been reported in some person and pet relationships. A better understanding of issues surrounding the end of the partnership, including the human-animal bond, will help inform the guide dog industry of how best to support their clients during this time and when transitioning to another dog. Findings may be applied to other service/assistance dog users and the pet owning community.</em></small><br>[ID: 33969026]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.physbeh.2014.04.045>Short term consequences of preventing visitor access to kennels on noise and the behaviour and physiology of dogs housed in a rescue shelter.</a></strong><br>2014. Hewison LF, Wright HF, Zulch HE, Ellis SL<br><small><strong>Abstract:</strong> <em>Re-homing centres present a range of potential stressors to kennelled dogs which are likely to impact negatively on their welfare. Despite the presence of visitors to the kennel often being considered a potential stressor, empirical investigation into their impact on the behaviour and welfare of kennelled dogs in re-homing centres is lacking. This study investigated the influence of changing visitor access policy from open access to prohibited viewing at kennels (with organised single meetings for viewing dogs outside of the kennel environment) on the welfare of 15 dogs housed in a dog-only re-homing facility. Data were collected across a number of domains comprising kennel noise levels, behavioural measures (activity, repetitive behaviour, response to human approach); physiological measures (urinary cortisol:creatinine ratios); sickness events and faecal scoring. The general kennel noise levels were significantly lower when visitor access to the kennel area was restricted. Furthermore, dogs were found to display behaviour indicative of improved welfare during this time period; dogs spent significantly more time sedentary, less time moving and exhibited significantly fewer episodes of repetitive behaviours. No significant change was seen in the urinary cortisol:creatinine ratio, nor in sickness behaviour, faecal scoring or response to a human approach test. Overall, the results from this study suggest that restricting visitors from viewing the dogs while in their kennels may be better for the dogs' short term welfare. </em></small><br>[ID: 24813703]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1098/rsos.181555>Pawsitively sad: pet-owners are more sensitive to negative emotion in animal distress vocalizations.</a></strong><br>2019. Parsons CE, LeBeau RT, Kringelbach ML, Young KS<br><small><strong>Abstract:</strong> <em>Pets have numerous, effective methods to communicate with their human hosts. Perhaps most conspicuous of these are distress vocalizations: in cats, the 'miaow' and in dogs, the 'whine' or 'whimper'. We compared a sample of young adults who owned cats and or dogs ('pet-owners' </em></small><br>[ID: 31598218]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1098/rsos.180491>Goats prefer positive human emotional facial expressions.</a></strong><br>2018. Nawroth C, Albuquerque N, Savalli C, Single MS, McElligott AG<br><small><strong>Abstract:</strong> <em>Domestication has shaped the physiology and the behaviour of animals to better adapt to human environments. Therefore, human facial expressions may be highly informative for animals domesticated for working closely with people, such as dogs and horses. However, it is not known whether other animals, and particularly those domesticated primarily for production, such as goats, are capable of perceiving human emotional cues. In this study, we investigated whether goats can distinguish human facial expressions when simultaneously shown two images of an unfamiliar human with different emotional valences (positive/happy or negative/angry). Both images were vertically attached to a wall on one side of a test arena, 1.3 m apart, and goats were released from the opposite side of the arena (distance of 4.0 m) and were free to explore and interact with the stimuli during the trials. Each of four test trials lasted 30 s. Overall, we found that goats preferred to interact first with happy faces, meaning that they are sensitive to human facial emotional cues. Goats interacted first, more often and for longer duration with positive faces when they were positioned on the right side. However, no preference was found when the positive faces were placed on the left side. We show that animals domesticated for production can discriminate human facial expressions with different emotional valences and prefer to interact with positive ones. Therefore, the impact of domestication on animal cognitive abilities may be more far-reaching than previously assumed.</em></small><br>[ID: 30225038]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3758/s13420-018-0352-z>Separate brain areas for processing human and dog faces as revealed by awake fMRI in dogs (Canis familiaris).</a></strong><br>2018. Thompkins AM, Ramaiahgari B, Zhao S, Gotoor SSR, Waggoner P, Denney TS, Deshpande G, Katz JS<br><small><strong>Abstract:</strong> <em>Functional magnetic resonance imaging (fMRI) has emerged as a viable method to study the neural processing underlying cognition in awake dogs. Working dogs were presented with pictures of dog and human faces. The human faces varied in familiarity (familiar trainers and unfamiliar individuals) and emotional valence (negative, neutral, and positive). Dog faces were familiar (kennel mates) or unfamiliar. The findings revealed adjacent but separate brain areas in the left temporal cortex for processing human and dog faces in the dog brain. The human face area (HFA) and dog face area (DFA) were both parametrically modulated by valence indicating emotion was not the basis for the separation. The HFA and DFA were not influenced by familiarity. Using resting state fMRI data, functional connectivity networks (connectivity fingerprints) were compared and matched across dogs and humans. These network analyses found that the HFA mapped onto the human fusiform area and the DFA mapped onto the human superior temporal gyrus, both core areas in the human face processing system. The findings provide insight into the evolution of face processing.</em></small><br>[ID: 30349971]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fvets.2019.00152>The Canine Frustration Questionnaire-Development of a New Psychometric Tool for Measuring Frustration in Domestic Dogs (<i>Canis familiaris</i>).</a></strong><br>2019. McPeake KJ, Collins LM, Zulch H, Mills DS<br><small><strong>Abstract:</strong> <em> Introduction: Psychometric tools have been developed for the assessment of behavioral and affective traits in non-human animals. Frustration can be defined as an emotional reaction experienced after a given expectation is violated. Frustration is a negative emotional state and whilst it probably plays a key role in certain behavior problems in dogs (e.g., aggressive behaviors), there appears to have been little attempt to scale this affective tendency. Therefore, the aim of the current study was to develop a tool to assess frustration tendencies in dogs. Materials and Methods: An online owner survey was developed. Items covered demographics, the training/behavioral history of the dog, and 33 frustration related items scored using a 5-point Likert scale. The questionnaire was disseminated via on-line channels over a 5-month period. Two thousand three hundred forty-eight respondents completed the questionnaire. Of these, 273 respondents completed it a second time 6 weeks later, and a separate 276 respondents completed it a second time 1 year later. Additionally, 92 paired responses were collected where two carers completed the questionnaire independently about the same dog. Intra- and inter-rater reliabilities were assessed prior to structuring the items using principal component analysis (PCA) with a Varimax rotation. Items were retained if they loaded &gt; 0.4 on at least one of the components extracted using the Kaiser criterion. Results: Twenty-two items were deemed to be reliable enough to be used in the PCA and 21 items loaded on a biologically meaningful 5-principal component solution. There was a significant positive correlation between each principal component and the owners' general perception of their dogs' frustration tendencies, alongside other expected correlates. Conclusion: This is the first reliable psychometric tool for the assessment of frustration in dogs-the Canine Frustration Questionnaire (CFQ). Further validation with behavioral tests and physiological measures is ongoing. </em></small><br>[ID: 31165075]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2013.00886>Socio-emotional correlates of a schooldog-teacher-team in the classroom.</a></strong><br>2013. Beetz A<br><small><strong>Abstract:</strong> <em>A growing number of teachers in Europe regularly take their dogs with them into the classroom. Limited research points at positive socio-emotional effects of this practice. In this study the effects of a schooldog-teacher-team on socioemotional experiences in school, depression and emotion regulation strategies were investigated in a classroom of third-graders (male n = 12, female n = 13), which had a schooldog present for 1 day per week in comparison with a control class (male n = 11, female n = 10). In contrast to the control class, the dog-class students reported a stronger improvement with regard to positive attitude toward school (repeated measures ANOVA; F = 10.769, df = 1, p = 0.002) and positive emotions related to learning (F = 4.479, df = 1, p = 0.042) over the course of the year. Since a prerequisite of all kinds of effective learning is a positive attitude and mood toward school and learning, the presence of a schooldog-teacher team thus has the potential to support learning. </em></small><br>[ID: 24348440]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s11055-005-0171-5>Evoked activity in the hypothalamus and amygdala of the cat in conditions of food-related motivation and emotional tension.</a></strong><br>2006. Pavlova IV, Vanetsian GL<br><small><strong>Abstract:</strong> <em>The amplitude-time characteristics of potentials evoked by clicks were analyzed in bilateral leads from the lateral hypothalamus and amygdala in cats in conditions of food-related motivation, emotional tension (presentation of dogs), and orientational reactions. In conditions of food-related motivation, as compared with the satiated state, there were decreases in the latent periods and changes in the amplitudes of the P1 and N2 components in the hypothalamus and P1, N2, and N3 in the amygdala. The most marked changes occurred on the left side in both structures. Presentation of dogs induced decreases in the latent periods of all components (including N1) of evoked potentials in the hypothalamus and amygdala, the most marked changes in the hypothalamus occurring on the right side and the most marked changes in the amygdala occurring on the left side. Conversely, orientational reactions to emotionally neutral stimuli induced increases in the latent periods of evoked potentials. It is concluded that there is an increase in sensory reactivity in the hypothalamus and amygdala in motivational-emotional states. It is suggested that the side of dominance in these structures may be associated both with the factor of the activity/passivity of the behavior in conditions of fear and the genesis of the emotion (motivational or informational).</em></small><br>[ID: 16380826]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ani9110887>Inter- and Intra-Species Communication of Emotion: Chemosignals as the Neglected Medium.</a></strong><br>2019. Semin GR, Scandurra A, Baragli P, Lanat A, D'Aniello B<br><small><strong>Abstract:</strong> <em>Human body odors contain chemosignals that make species-specific communication possible. Such communication is without communicative intent and is generally below the threshold of consciousness. Human recipients of these chemosignals produced during emotional conditions display a simulacrum of the emotional state under which the chemosignal was produced. The investigation of an inter-species transfer of emotions via chemosignals was initiated by considerations of the historically anchored interdependence between humans and domesticated species, such as dogs and horses. Indeed, experiments with dogs have demonstrated that human body odors produced under emotional conditions of happiness and fear led dogs to manifest corresponding emotions to those experienced by humans. Preliminary data from horses also show that human body odors collected under fear and happiness conditions activate the autonomic nervous system of horses differentially. These studies indicate the possibility of a road to open our understanding of inter-species emotional communication via chemosignals.</em></small><br>[ID: 31683710]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-017-1139-x>Interspecies transmission of emotional information via chemosignals: from humans to dogs (Canis lupus familiaris).</a></strong><br>2018. D'Aniello B, Semin GR, Alterisio A, Aria M, Scandurra A<br><small><strong>Abstract:</strong> <em>We report a study examining interspecies emotion transfer via body odors (chemosignals). Do human body odors (chemosignals) produced under emotional conditions of happiness and fear provide information that is detectable by pet dogs (Labrador and Golden retrievers)? The odor samples were collected from the axilla of male donors not involved in the main experiment. The experimental setup involved the co-presence of the dog's owner, a stranger and the odor dispenser in a space where the dogs could move freely. There were three odor conditions [fear, happiness, and control (no sweat)] to which the dogs were assigned randomly. The dependent variables were the relevant behaviors of the dogs (e.g., approaching, interacting and gazing) directed to the three targets (owner, stranger, sweat dispenser) aside from the dogs' stress and heart rate indicators. The results indicated with high accuracy that the dogs manifested the predicted behaviors in the three conditions. There were fewer and shorter owner directed behaviors and more stranger directed behaviors when they were in the \"happy odor condition\" compared to the fear odor and control conditions. In the fear odor condition, they displayed more stressful behaviors. The heart rate data in the control and happy conditions were significantly lower than in the fear condition. Our findings suggest that interspecies emotional communication is facilitated by chemosignals.</em></small><br>[ID: 28988316]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0051775>Human perception of fear in dogs varies according to experience with dogs.</a></strong><br>2012. Wan M, Bolger N, Champagne FA<br><small><strong>Abstract:</strong> <em>To investigate the role of experience in humans' perception of emotion using canine visual signals, we asked adults with various levels of dog experience to interpret the emotions of dogs displayed in videos. The video stimuli had been pre-categorized by an expert panel of dog behavior professionals as showing examples of happy or fearful dog behavior. In a sample of 2,163 participants, the level of dog experience strongly predicted identification of fearful, but not of happy, emotional examples. The probability of selecting the \"fearful\" category to describe fearful examples increased with experience and ranged from.30 among those who had never lived with a dog to greater than.70 among dog professionals. In contrast, the probability of selecting the \"happy\" category to describe happy emotional examples varied little by experience, ranging from.90 to.93. In addition, the number of physical features of the dog that participants reported using for emotional interpretations increased with experience, and in particular, more-experienced respondents were more likely to attend to the ears. Lastly, more-experienced respondents provided lower difficulty and higher accuracy self-ratings than less-experienced respondents when interpreting both happy and fearful emotional examples. The human perception of emotion in other humans has previously been shown to be sensitive to individual differences in social experience, and the results of the current study extend the notion of experience-dependent processes from the intraspecific to the interspecific domain.</em></small><br>[ID: 23284765]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ani11061504>An Animal-Assisted Education Intervention with Dogs to Promote Emotion Comprehension in Primary School Children-The Federico II Model of Healthcare Zooanthropology.</a></strong><br>2021. Scandurra C, Santaniello A, Cristiano S, Mezza F, Garzillo S, Pizzo R, Menna LF, Bochicchio V<br><small><strong>Abstract:</strong> <em>Emotion comprehension (EC) is a crucial competence for children, as it determines the quality of peer interactions. This study assessed the efficacy of an animal-assisted education (AAE) intervention with dogs based on the Federico II Model of Healthcare Zooanthropology (FMHZ) to promote EC in a group of primary school children. One hundred and four children (48 females) aged 6-7 years took part in the study, of whom 63 participated in the AAE intervention (i.e., experimental group) and 41 did not (i.e., control group). The intervention was deployed in a school setting through a group format and consisted of five bimonthly sessions. EC was assessed pre- and post-intervention, and at a 3-month follow-up. Student's t-test and mixed-model ANOVA were performed to analyze the effect of the intervention on EC. EC significantly improved in children of the experimental group compared to the control group. Significant time effects from pre- to post-intervention, post-intervention to follow-up, and pre-intervention to follow-up assessment were found in the experimental group only. AAE based on FMHZ was effective in improving EC in children.</em></small><br>[ID: 34067357]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0170730>Human Empathy, Personality and Experience Affect the Emotion Ratings of Dog and Human Facial Expressions.</a></strong><br>2017. Kujala MV, Somppi S, Jokela M, Vainio O, Parkkonen L<br><small><strong>Abstract:</strong> <em>Facial expressions are important for humans in communicating emotions to the conspecifics and enhancing interpersonal understanding. Many muscles producing facial expressions in humans are also found in domestic dogs, but little is known about how humans perceive dog facial expressions, and which psychological factors influence people's perceptions. Here, we asked 34 observers to rate the valence, arousal, and the six basic emotions (happiness, sadness, surprise, disgust, fear, and anger/aggressiveness) from images of human and dog faces with Pleasant, Neutral and Threatening expressions. We investigated how the subjects' personality (the Big Five Inventory), empathy (Interpersonal Reactivity Index) and experience of dog behavior affect the ratings of dog and human faces. Ratings of both species followed similar general patterns: human subjects classified dog facial expressions from pleasant to threatening very similarly to human facial expressions. Subjects with higher emotional empathy evaluated Threatening faces of both species as more negative in valence and higher in anger/aggressiveness. More empathetic subjects also rated the happiness of Pleasant humans but not dogs higher, and they were quicker in their valence judgments of Pleasant human, Threatening human and Threatening dog faces. Experience with dogs correlated positively with ratings of Pleasant and Neutral dog faces. Personality also had a minor effect on the ratings of Pleasant and Neutral faces in both species. The results imply that humans perceive human and dog facial expression in a similar manner, and the perception of both species is influenced by psychological factors of the evaluators. Especially empathy affects both the speed and intensity of rating dogs' emotional facial expressions.</em></small><br>[ID: 28114335]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ijerph17020506>Can Children of Different Ages Recognize Dog Communication Signals in Different Situations?</a></strong><br>2020. Eretov P, Chaloupkov H, Hefferov M, Jozfkov E<br><small><strong>Abstract:</strong> <em>The presented study examines the ability of 265 children aged 4-12 years to correctly assign contextual cues and inner state values to a set of audio and audio-visual recordings of dog vocalizations and behaviors in different situations. Participants were asked to mark which situation each recording captured, what inner state of the dog it showed, and what inner state a human would feel in the same situation. Recognition of the inner state of dogs was affected by the age of the child when evaluating the audio recordings (</em></small><br>[ID: 31941151]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1046/j.1440-1681.2000.03370.x>Heart-lung interactions: the sigh and autonomic control in the bronchial and coronary circulations.</a></strong><br>2000. Porges WL, Hennessy EJ, Quail AW, Cottee DB, Moore PG, McIlveen SA, Parsons GH, White SW<br><small><strong>Abstract:</strong> <em>1. The Darwin hypothesis that human and animal expressions of emotion are the product of evolution and are tied to patterns of autonomic activity specified to progress the emotion remains under challenge. 2. The sigh is a respiratory behaviour linked with emotional expression in animals and humans from birth to death. The aim of the present study was to explore Darwin's hypothesis with respect to tied autonomic activity underlying sigh-induced changes in the bronchial and coronary circulations. 3. Awake dogs were prepared using pulsed ultrasonic flow probes on the right bronchial artery, parent intercostal artery and brachial artery, or on the right, circumflex and anterior descending coronary arteries. Central venous (CVP) and arterial pressures (AP) were measured; heart rate and flow conductances were derived. Three spontaneous sighs were monitored before and during random blockade of individual and combinations of cholinoceptors, alpha-adrenoceptors and beta-adrenoceptors using methscopolamine, phentolamine and propranolol infusions. The data were subject to a 2(3) factorial analysis. 4. A spontaneous sigh is marked by a transient fall and return (< 3 s) in CVP of 18 mmHg (from 4 +/- 1 to -14 +/- 2 mmHg), usually followed by apnoea lasting 23 +/- 2 s. There is an immediate tachycardia and small rise in AP (phase 1) then, during apnoea, bradycardia and a fall in AP (phase 2). During phase 2, bronchial and coronary blood flow and conductance rise two- to three-fold over 30s (peak at 8s). The vascular changes are absent in parent intercostal and brachial beds.</em></small><br>[ID: 11117224]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-019-52938-4>The ability to recognize dog emotions depends on the cultural milieu in which we grow up.</a></strong><br>2019. Amici F, Waterman J, Kellermann CM, Karimullah K, Bruer J<br><small><strong>Abstract:</strong> <em>Inter-specific emotion recognition is especially adaptive when species spend a long time in close association, like dogs and humans. Here, we comprehensively studied the human ability to recognize facial expressions associated with dog emotions (hereafter, emotions). Participants were presented with pictures of dogs, humans and chimpanzees, showing angry, fearful, happy, neutral and sad emotions, and had to assess which emotion was shown, and the context in which the picture had been taken. Participants were recruited among children and adults with different levels of general experience with dogs, resulting from different personal (i.e. dog ownership) and cultural experiences (i.e. growing up or being exposed to a cultural milieu in which dogs are highly valued and integrated in human lives). Our results showed that some dog emotions such as anger and happiness are recognized from early on, independently of experience. However, the ability to recognize dog emotions is mainly acquired through experience. In adults, the probability of recognizing dog emotions was higher for participants grown up in a cultural milieu with a positive attitude toward dogs, which may result in different passive exposure, interest or inclination toward this species.</em></small><br>[ID: 31712680]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ani10060927>Reliability of Family Dogs' Sleep Structure Scoring Based on Manual and Automated Sleep Stage Identification.</a></strong><br>2020. Gergely A, Kiss O, Reicher V, Iotchev I, Kovcs E, Gombos F, Benczr A, Galambos , Topl J, Kis A<br><small><strong>Abstract:</strong> <em>Non-invasive polysomnography recording on dogs has been claimed to produce data comparable to those for humans regarding sleep macrostructure, EEG spectra and sleep spindles. While functional parallels have been described relating to both affective (e.g., emotion processing) and cognitive (e.g., memory consolidation) domains, methodologically relevant questions about the reliability of sleep stage scoring still need to be addressed. In Study 1, we analyzed the effects of different coders and different numbers of visible EEG channels on the visual scoring of the same polysomnography recordings. The lowest agreement was found between independent coders with different scoring experience using full (3 h-long) recordings of the whole dataset, and the highest agreement within-coder, using only a fraction of the original dataset (randomly selected 100 epochs (i.e., 100  20 s long segments)). The identification of drowsiness was found to be the least reliable, while that of non-REM (rapid eye movement, NREM) was the most reliable. Disagreements resulted in no or only moderate differences in macrostructural and spectral variables. Study 2 targeted the task of automated sleep EEG time series classification. Supervised machine learning (ML) models were used to help the manual annotation process by reliably predicting if the dog was sleeping or awake. Logistic regression models (LogREG), gradient boosted trees (GBT) and convolutional neural networks (CNN) were set up and trained for sleep state prediction from already collected and manually annotated EEG data. The evaluation of the individual models suggests that their combination results in the best performance: ~0.9 AUC test scores.</em></small><br>[ID: 32466600]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1186/s12917-015-0373-1>Dogs with separation-related problems show a \"less pessimistic\" cognitive bias during treatment with fluoxetine (Reconcile) and a behaviour modification plan.</a></strong><br>2015. Karagiannis CI, Burman OH, Mills DS<br><small><strong>Abstract:</strong> <em>Canine separation-related problems (SRP) (also described as \"separation anxiety\" or \"separation distress\") are among the most common behavioural complaints of dog owners. Treatment with psychoactive medication in parallel with a behaviour modification plan is well documented in the literature, but it is unknown if this is associated with an improvement in underlying affective state (emotion and mood) or simply an inhibition of the behaviour. Cognitive judgement bias tasks have been proposed as a method for assessing underlying affective state and so we used this approach to identify if any change in clinical signs during treatment was associated with a consistent change in cognitive bias (affective state). Five dogs showing signs of SRP (vocalising - e.g. barking, howling-, destruction of property, and toileting - urination or defecation- when alone) were treated with fluoxetine chewable tablets (Reconcile) and set on a standard behaviour modification plan for two months. Questionnaires and interviews of the owners were used to monitor the clinical progress of the dogs. Subjects were also evaluated using a spatial cognitive bias test to infer changes in underlying affect prior to, and during, treatment. Concurrently, seven other dogs without signs of SRP were tested in the same way to act as controls. Furthermore, possible correlations between cognitive bias and clinical measures were also assessed for dogs with SRP.</em></small><br>[ID: 25889323]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2021.615074>Do Emotional Cues Influence the Performance of Domestic Dogs in an Observational Learning Task?</a></strong><br>2021. Albuquerque N, Savalli C, Cabral F, Resende B<br><small><strong>Abstract:</strong> <em>Using social information is not indiscriminate and being able to choose what to copy and from whom to copy is critical. Dogs are able to learn socially, to recognize, and respond to dog as well as human emotional expressions, and to make reputation-like inferences based on how people behave towards their owner. Yet, the mechanisms dogs use for obtaining and utilizing social information are still to be fully understood, especially concerning whether emotional cues influence dogs' social learning. Therefore, our main aim was to test the hypothesis that an emotionally charged (negative, positive, or neutral) interaction with the demonstrator of a \"V\" detour task prior to testing would affect subjects' performance, by: (i) changing the value of the information provided by the demonstrator or (ii) changing the valence of the learning environment. Our experimental design consisted of three phases: pre-test (subjects were allowed to solve the task alone); emotional display (dogs watched the unfamiliar human behaving in either a positive, negative or neutral way towards their owner); test (demonstrator showed the task and subjects were allowed to move freely). Only dogs that failed in pre-test were considered for analysis (</em></small><br>[ID: 34093306]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.neulet.2015.07.020>Perceiving emotions in human-human and human-animal interactions: Hemodynamic prefrontal activity (fNIRS) and empathic concern.</a></strong><br>2015. Vanutelli ME, Balconi M<br><small><strong>Abstract:</strong> <em>In the last years social neuroscience research attempted to identify the neural networks underlying the human ability to perceive others' emotions, a core process in establishing meaningful social bonds. A large amount of papers arose and identified common and specific empathy-based networks with respect to stimulus type and task. Despite the great majority of studies focused on human-human contexts, we do not establish relations with only other humans, but also with non-human animals. The aim of the present work was to explore the brain mechanisms involved in empathic concern for people who interacts with both peers and other species. Participants have been assessed by functional near-infrared spectroscopy (fNIRS) while viewing pictures depicting humans interacting with both other men and women (human-human condition: HH), or with dogs and cats (human-animal: HA). Results showed that aggressive HH interactions elicited greater prefrontal activity (PFC) than HA ones while, when considering HA interactions, friendly ones were related to higher cortical activity. Finally, oxy (O2Hb) and deoxyhemoglobin (HHb) increasing related to the processing of aggressive interactions positively correlated with different empathic measures, within more specific brain regions. Results were elucidated with respect to available evidence on emotion perception, empathic neural mechanisms and their functional meaning for human-animal contexts. </em></small><br>[ID: 26272301]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-021-01487-3>Horses are sensitive to baby talk: pet-directed speech facilitates communication with humans in a pointing task and during grooming.</a></strong><br>2021. Lansade L, Trsch M, Parias C, Blanchard A, Gorosurreta E, Calandreau L<br><small><strong>Abstract:</strong> <em>Pet-directed speech (PDS) is a type of speech humans spontaneously use with their companion animals. It is very similar to speech commonly used when talking to babies. A survey on social media showed that 92.7% of the respondents used PDS with their horse, but only 44.4% thought that their horse was sensitive to it, and the others did not know or doubted its efficacy. We, therefore, decided to test the impact of PDS on two tasks. During a grooming task that consisted of the experimenter scratching the horse with their hand, the horses (n=20) carried out significantly more mutual grooming gestures toward the experimenter, looked at the person more, and moved less when spoken to with PDS than with Adult-directed speech (ADS). During a pointing task in which the experimenter pointed at the location of a reward with their finger, horses who had been spoken to with PDS (n=10) found the food significantly more often than chance, which was not the case when horses were spoken to with ADS (n=10). These results thus indicate that horses, like certain non-human primates and dogs are sensitive to PDS. PDS could thus foster communication between people and horses during everyday interactions.</em></small><br>[ID: 33738670]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1177/0956797620979149>Dogs Mentally Represent Jealousy-Inducing Social Interactions.</a></strong><br>2021. Bastos APM, Neilands PD, Hassall RS, Lim BC, Taylor AH<br><small><strong>Abstract:</strong> <em>Jealousy may have evolved to protect valuable social bonds from interlopers, but some researchers have suggested that it is linked to self-awareness and theory of mind, leading to claims that it is unique to humans. We presented dogs (</em></small><br>[ID: 33825583]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-018-26892-6>Cross-modal perception of human emotion in domestic horses (Equus caballus).</a></strong><br>2018. Nakamura K, Takimoto-Inose A, Hasegawa T<br><small><strong>Abstract:</strong> <em>Humans have domesticated many kinds of animals in their history. Dogs and horses have particularly close relationships with humans as cooperative partners. However, fewer scientific studies have been conducted on cognition in horses compared to dogs. Studies have shown that horses cross-modally distinguish human facial expressions and recognize familiar people, which suggests that they also cross-modally distinguish human emotions. In the present study, we used the expectancy violation method to investigate whether horses cross-modally perceive human emotions. Horses were shown a picture of a human facial expression on a screen, and they then heard a human voice from the speaker before the screen. The emotional values of the visual and auditory stimuli were the same in the congruent condition and different in the incongruent condition. Horses looked at the speaker significantly longer in the incongruent condition than in the congruent condition when they heard their caretaker's voices but not when they heard the stranger voice. In addition, they responded significantly more quickly to the voice in the incongruent condition than in the congruent one. To the best of our knowledge, this is the first study to show that horses cross-modally recognized the emotional states of their caretakers and strangers.</em></small><br>[ID: 29930289]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-021-01475-7>Dog-human social relationship: representation of human face familiarity and emotions in the dog brain.</a></strong><br>2021. Thompkins AM, Lazarowski L, Ramaiahgari B, Gotoor SSR, Waggoner P, Denney TS, Deshpande G, Katz JS<br><small><strong>Abstract:</strong> <em>This study investigated the behavioral and neural indices of detecting facial familiarity and facial emotions in human faces by dogs. Awake canine fMRI was used to evaluate dogs' neural response to pictures and videos of familiar and unfamiliar human faces, which contained positive, neutral, and negative emotional expressions. The dog-human relationship was behaviorally characterized out-of-scanner using an unsolvable task. The caudate, hippocampus, and amygdala, mainly implicated in reward, familiarity and emotion processing, respectively, were activated in dogs when viewing familiar and emotionally salient human faces. Further, the magnitude of activation in these regions correlated with the duration for which dogs showed human-oriented behavior towards a familiar (as opposed to unfamiliar) person in the unsolvable task. These findings provide a bio-behavioral basis for the underlying markers and functions of human-dog interaction as they relate to familiarity and emotion in human faces.</em></small><br>[ID: 33598770]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ani10010164>Social Referencing in the Domestic Horse.</a></strong><br>2020. Schrimpf A, Single MS, Nawroth C<br><small><strong>Abstract:</strong> <em>Dogs and cats use human emotional information directed to an unfamiliar situation to guide their behavior, known as social referencing. It is not clear whether other domestic species show similar socio-cognitive abilities in interacting with humans. We investigated whether horses (</em></small><br>[ID: 31963699]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.physbeh.2016.03.019>How good is this food? A study on dogs' emotional responses to a potentially pleasant event using infrared thermography.</a></strong><br>2016. Travain T, Colombo ES, Grandi LC, Heinzl E, Pelosi A, Prato Previde E, Valsecchi P<br><small><strong>Abstract:</strong> <em>Understanding how animals express positive emotions is becoming an interesting and promising area of research in the study of animal emotions and affective experiences. In the present study, we used infrared thermography in combination with behavioral measures, heart rate (HR) and heart rate variability (HRV), to investigate dogs' emotional responses to a potentially pleasant event: receiving palatable food from the owner. Nineteen adult pet dogs, 8 females and 11 males, were tested and their eye temperature, HR, HRV and behavior were recorded during a 30-minutestestconsisting of three 10-minute consecutive phases: Baseline (Phase 1), positive stimulation through the administration of palatable treats (Feeding, Phase 2) and Post-feeding condition following the positive stimulation (Phase 3). Dogs' eye temperature and mean HR significantly increased during the positive stimulation phase compared with both Baseline and Post-feeding phases. During the positive stimulation with food (Phase 2), dogs engaged in behaviors indicating a positive emotional state and a high arousal, being focused on food treats and increasing tail wagging. However, there was no evidence of an increase in HRV during Phase 2 compared to the Phase 1, with SDNN significantly increasing only in Phase 3, after the positive stimulation occurred. Overall results point out that IRT may be a useful tool in assessing emotional states in dogs in terms of arousal but fails to discriminate emotional valence, whose interpretation cannot disregard behavioral indexes. </em></small><br>[ID: 26996276]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-021-01532-1>Evaluating the accuracy of facial expressions as emotion indicators across contexts in dogs.</a></strong><br>2021. Bremhorst A, Mills DS, Wrbel H, Riemer S<br><small><strong>Abstract:</strong> <em>Facial expressions potentially serve as indicators of animal emotions if they are consistently present across situations that (likely) elicit the same emotional state. In a previous study, we used the Dog Facial Action Coding System (DogFACS) to identify facial expressions in dogs associated with conditions presumably eliciting positive anticipation (expectation of a food reward) and frustration (prevention of access to the food). Our first aim here was to identify facial expressions of positive anticipation and frustration in dogs thatare context-independent (and thus have potential as emotion indicators) and to distinguish them fromexpressions thatare reward-specific(and thus mightrelate to amotivational state associated with the expectedreward). Therefore, we tested a new sample of 28 dogs with a similar set-up designed to induce positive anticipation (positive condition) and frustration (negative condition) in two reward contexts: food and toys. The previous results were replicated: Ears adductor was associated with the positive condition and Ears flattener, Blink, Lips part, Jaw drop, and Nose lick with the negative condition. Four additional facial actions were also more common in the negative condition. All actions except the Upper lip raiser were independent of reward type. Our second aim was to assess basic measures of diagnostic accuracy for the potential emotion indicators. Ears flattener and Ears downward had relatively high sensitivity but low specificity, whereas the opposite was the case for the other negative correlates. Ears adductor had excellent specificity but low sensitivity. If the identifiedfacial expressions were to be used individually as diagnostic indicators, none would allow consistent correct classifications of the associated emotion. Diagnostic accuracy measures are an essential feature for validity assessments of potential indicators of animal emotion.</em></small><br>[ID: 34338869]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.yhbeh.2011.05.012>Urinary oxytocin as a noninvasive biomarker of positive emotion in dogs.</a></strong><br>2011. Mitsui S, Yamamoto M, Nagasawa M, Mogi K, Kikusui T, Ohtani N, Ohta M<br><small><strong>Abstract:</strong> <em>A reliable assay based on physiological parameters that does not require subjective input from the owners is required to assess positive emotions in dogs. In addition, when viewed from an animal welfare perspective, physiological parameters should be collected in a noninvasive manner. Oxytocin (OT) is a biomarker that may be associated with a calm, relaxed state, and positive emotion. We measured the time-lapse in the concentration of plasma OT relative to urinary OT using a radioimmunoassay with sufficient sensitivity and low variability, and examined the relationship between OT and cortisol. Six dogs were injected with exogenous OT intravenously to increase the blood OT concentration. As a result, the highest concentration of urinary OT occurred 1h after the injection, although there was little change in urinary cortisol. Moreover, to evaluate the influence of stimuli on urinary OT and cortisol, we provided three stimuli of eating food, exercising and stroking, all of which were assumed to inspire a positive emotion in dogs, and significantly increased urinary OT concentrations. Our findings indicate that urinary OT might be useful as a noninvasive and objective biomarker of positive emotion in dogs.</em></small><br>[ID: 21689655]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1136/medethics-2016-103630>Advances in neuroscience imply that harmful experiments in dogs are unethical.</a></strong><br>2018. Bailey J, Pereira S<br><small><strong>Abstract:</strong> <em>Functional MRI (fMRI) of fully awake and unrestrained dog 'volunteers' has been proven an effective tool to understand the neural circuitry and functioning of the canine brain. Although every dog owner would vouch that dogs are perceptive, cognitive, intuitive and capable of positive emotions/empathy, as indeed substantiated by ethological studies for some time, neurological investigations now corroborate this. These studies show that there exists a striking similarity between dogs and humans in the functioning of the caudate nucleus (associated with pleasure and emotion), and dogs experience positive emotions, empathic-like responses and demonstrate human bonding which, some scientists claim, may be at least comparable with human children. There exists an area analogous to the 'voice area' in the canine brain, enabling dogs to comprehend and respond to emotional cues/valence in human voices, and evidence of a region in the temporal cortex of dogs involved in the processing of faces, as also observed in humans and monkeys. We therefore contend that using dogs in invasive and/or harmful research, and toxicity testing, cannot be ethically justifiable.</em></small><br>[ID: 28739639]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0192846>Associations between owner personality and psychological status and the prevalence of canine behavior problems.</a></strong><br>2018. Dodman NH, Brown DC, Serpell JA<br><small><strong>Abstract:</strong> <em>Behavioral problems are a major source of poor welfare and premature mortality in companion dogs. Previous studies have demonstrated associations between owners' personality and psychological status and the prevalence and/or severity of their dogs' behavior problems. However, the mechanisms responsible for these associations are currently unknown. Other studies have detected links between the tendency of dogs to display behavior problems and their owners' use of aversive or confrontational training methods. This raises the possibility that the effects of owner personality and psychological status on dog behavior are mediated via their influence on the owner's choice of training methods. We investigated this hypothesis in a self-selected, convenience sample of 1564 current dog owners using an online battery of questionnaires designed to measure, respectively, owner personality, depression, emotion regulation, use of aversive/confrontational training methods, and owner-reported dog behavior. Multivariate linear and logistic regression analyses identified modest, positive associations between owners' use of aversive/confrontational training methods and the prevalence/severity of the following dog behavior problems: owner-directed aggression, stranger-directed aggression, separation problems, chasing, persistent barking, and house-soiling (urination and defecation when left alone). The regression models also detected modest associations between owners' low scores on four of the 'Big Five' personality dimensions (Agreeableness, Emotional Stability, Extraversion & Conscientiousness) and their dogs' tendency to display higher rates of owner-directed aggression, stranger-directed fear, and/or urination when left alone. The study found only weak evidence to support the hypothesis that these relationships between owner personality and dog behavior were mediated via the owners' use of punitive training methods, but it did detect a more than five-fold increase in the use of aversive/confrontational training techniques among men with moderate depression. Further research is needed to clarify the causal relationship between owner personality and psychological status and the behavioral problems of companion dogs.</em></small><br>[ID: 29444154]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1027/1618-3169/a000515>When Dogs Shrink the Typical Lengthening Effect Caused by Negative Emotions.</a></strong><br>2021. Hallez Q, Baltenneck N, Galiano AR<br><small><strong>Abstract:</strong> <em> This paper examines how dogs can modulate the effects of emotion on time perception. To this end, participants performed a temporal bisection task with stimulus durations presented in the form of neutral or emotional facial expressions (angry, sad, and happy faces). In the first experiment, dog owners were compared with nondog owners, while in the second experiment, students were randomly assigned to one of the three waiting groups (waiting alone, with another person, or with a dog) before being confronted with the temporal bisection task. The results showed that dogs allowed the participants to regulate the intensity of negative emotional effects, while no statistical differences emerged for the happy facial expressions. In certain circumstances, dogs could even lead the subjects to generate underestimation of time when faced with negative facial expressions. </em></small><br>[ID: 34405692]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-020-01348-5>Perception of dynamic facial expressions of emotion between dogs and humans.</a></strong><br>2020. Correia-Caeiro C, Guo K, Mills DS<br><small><strong>Abstract:</strong> <em>Facial expressions are a core component of the emotional response of social mammals. In contrast to Darwin's original proposition, expressive facial cues of emotion appear to have evolved to be species-specific. Faces trigger an automatic perceptual process, and so, inter-specific emotion perception is potentially a challenge; since observers should not try to \"read\" heterospecific facial expressions in the same way that they do conspecific ones. Using dynamic spontaneous facial expression stimuli, we report the first inter-species eye-tracking study on fully unrestrained participants and without pre-experiment training to maintain attention to stimuli, to compare how two different species living in the same ecological niche, humans and dogs, perceive each other's facial expressions of emotion. Humans and dogs showed different gaze distributions when viewing the same facial expressions of either humans or dogs. Humans modulated their gaze depending on the area of interest (AOI) being examined, emotion, and species observed, but dogs modulated their gaze depending on AOI only. We also analysed if the gaze distribution was random across AOIs in both species: in humans, eye movements were not correlated with the diagnostic facial movements occurring in the emotional expression, and in dogs, there was only a partial relationship. This suggests that the scanning of facial expressions is a relatively automatic process. Thus, to read other species' facial emotions successfully, individuals must overcome these automatic perceptual processes and employ learning strategies to appreciate the inter-species emotional repertoire.</em></small><br>[ID: 32052285]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-020-79247-5>Exploring the dog-human relationship by combining fMRI, eye-tracking and behavioural measures.</a></strong><br>2020. Karl S, Boch M, Zamansky A, van der Linden D, Wagner IC, Vlter CJ, Lamm C, Huber L<br><small><strong>Abstract:</strong> <em>Behavioural studies revealed that the dog-human relationship resembles the human mother-child bond, but the underlying mechanisms remain unclear. Here, we report the results of a multi-method approach combining fMRI (N=17), eye-tracking (N=15), and behavioural preference tests (N=24) to explore the engagement of an attachment-like system in dogs seeing human faces. We presented morph videos of the caregiver, a familiar person, and a stranger showing either happy or angry facial expressions. Regardless of emotion, viewing the caregiver activated brain regions associated with emotion and attachment processing in humans. In contrast, the stranger elicited activation mainly in brain regions related to visual and motor processing, and the familiar person relatively weak activations overall. While the majority of happy stimuli led to increased activation of the caudate nucleus associated with reward processing, angry stimuli led to activations in limbic regions. Both the eye-tracking and preference test data supported the superior role of the caregiver's face and were in line with the findings from the fMRI experiment. While preliminary, these findings indicate that cutting across different levels, from brain to behaviour, can provide novel and converging insights into the engagement of the putative attachment system when dogs interact with humans.</em></small><br>[ID: 33335230]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ani9090630>Are Horses (<i>Equus caballus</i>) Sensitive to Human Emotional Cues?</a></strong><br>2019. Baba C, Kawai M, Takimoto-Inose A<br><small><strong>Abstract:</strong> <em>Emotions are important for social animals because animals' emotions function as beneficial cues to identify valuable resources such as food or to avoid danger by providing environmental information. Emotions also enable animals to predict individuals' behavior and determine how to behave in a specific context. Recently, several studies have reported that dogs are highly sensitive to not only conspecific but also human emotional cues. These studies suggest that domestication may have affected such sensitivity. However, there are still few studies that examine whether other domesticated animals, in addition to dogs, exhibit sensitivity to human emotional cues. In this study, we used a gaze-following task to investigate whether horses (</em></small><br>[ID: 31470656]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-019-55714-6>Differences in facial expressions during positive anticipation and frustration in dogs awaiting a reward.</a></strong><br>2019. Bremhorst A, Sutter NA, Wrbel H, Mills DS, Riemer S<br><small><strong>Abstract:</strong> <em>Facial expressions are considered sensitive indicators of emotional states in humans and many animals. Identifying facial indicators of emotion is a major challenge and little systematic research has been done in non-primate species. In dogs, such research is important not only to address fundamental and applied scientific questions but also for practical reasons, since many problem behaviours are assumed to have an emotional basis, e.g. aggression based on frustration. Frustration responses can occur in superficially similar contexts as the emotional state of positive anticipation. For instance, the anticipated delivery of a food reward may induce the state of positive anticipation, but over time, if the food is not delivered, this will be replaced by frustration. We examined dogs' facial expressions in contexts presumed to induce both positive anticipation and frustration, respectively, within a single controlled experimental setting. Using DogFACS, an anatomically-based method for coding facial expressions of dogs, we found that the \"Ears adductor\" actionwas more common in the positive condition and \"Blink\", \"Lips part\", \"Jaw drop\", \"Nose lick\", and \"Ears flattener\" were more common in the negative condition. This study demonstrates how differences in facial expression in emotionally ambiguous contexts may be used to help infer emotional states of different valence.</em></small><br>[ID: 31848389]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Animal illness and human emotion. Behavioral problems.</strong><br>1991. Borchelt PL<br><small><strong>Abstract:</strong> <em>Understanding companion animal behavior and treating behavior problems requires an appreciation of both the species-typical and individualistic nature of the behavior of dogs and cats, as well as people. Pet behavior problems are influenced by many factors, including genes, physiological processes and a myriad of environmental influences during the development of the pet throughout its life. Human behavior is influenced greatly by cognitive/linguistic capabilities unavailable to pets. Human anthropomorphism, egocentrism, and our tendency to attribute causality have surprisingly little effect on the development of major pet behavior problems but must be considered factors bearing on the successful treatment of these problems.</em></small><br>[ID: 1804488]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fnbeh.2017.00210>The Way Dogs (<i>Canis familiaris</i>) Look at Human Emotional Faces Is Modulated by Oxytocin. An Eye-Tracking Study.</a></strong><br>2017. Kis A, Herndi A, Miklsi B, Kanizsr O, Topl J<br><small><strong>Abstract:</strong> <em>Dogs have been shown to excel in reading human social cues, including facial cues. In the present study we used eye-tracking technology to further study dogs' face processing abilities. It was found that dogs discriminated between human facial regions in their spontaneous viewing pattern and looked most to the eye region independently of facial expression. Furthermore dogs played most attention to the first two images presented, afterwards their attention dramatically decreases; a finding that has methodological implications. Increasing evidence indicates that the oxytocin system is involved in dogs' human-directed social competence, thus as a next step we investigated the effects of oxytocin on processing of human facial emotions. It was found that oxytocin decreases dogs' looking to the human faces expressing angry emotional expression. More interestingly, however, after oxytocin pre-treatment dogs' preferential gaze toward the eye region when processing happy human facial expressions disappears. These results provide the first evidence that oxytocin is involved in the regulation of human face processing in dogs. The present study is one of the few empirical investigations that explore eye gaze patterns in nave and untrained pet dogs using a non-invasive eye-tracking technique and thus offers unique but largely untapped method for studying social cognition in dogs.</em></small><br>[ID: 29163082]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-014-0779-3>Fetching what the owner prefers? Dogs recognize disgust and happiness in human behaviour.</a></strong><br>2015. Turcsn B, Sznth F, Miklsi , Kubinyi E<br><small><strong>Abstract:</strong> <em>Research using the two-object choice paradigm showed that dogs prefer the object associated with the happy human emotion. However, they provided rather ambiguous results regarding the negative emotions. We assumed that differences between the dogs' and owners' interest towards the 'negative' object might be responsible for this. In our experiment, dogs observed their owner expressing different emotions towards two uniform plastic bottles. Five dog groups were tested based on the condition they received: (1) happy versus neutral, (2) happy versus disgust, (3) neutral versus disgust and (4-5) neutral vs neutral, as control groups. Contrary to previous studies using free choice paradigm, we used a task-driven approach. After the demonstration, the dogs had to retrieve one object to the owner. The dogs' performance in the two neutral-neutral groups did not differ from the chance level. In contrast, subjects were able to distinguish between the happy and neutral expression of the owner: they both approached and fetched the 'happy' object. In the happy-disgusted and neutral-disgusted groups, the dogs approached the bottles randomly, suggesting that they found the 'disgusting' and 'neutral' objects equally attractive. Nevertheless, the dogs preferentially retrieved the object marked with the relatively more positive emotion (happy or neutral) to the owner in both conditions. Our results demonstrate that dogs are able to recognize which is the more positive among two emotions, and in a fetching task situation, they override their own interest in the 'disgusting' object and retrieve what the owner prefers. </em></small><br>[ID: 24989132]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Physiology of REM sleep, cataplexy, and sleep paralysis.</strong><br>1995. Hishikawa Y, Shimizu T<br><small><strong>Abstract:</strong> <em>The main neural structures generating muscle atonia and other phenomena characteristic of REM sleep are present in dorsolateral portions of the pons in the brainstem. Occurrence of REM sleep and the NREM-REM sleep cycle are probably determined by a balance or interaction between the cholinergic and cholinoceptive REM sleep-on neuronal populations and the monoaminergic REM sleep-off neuronal population. Neural activities producing generalized muscle atonia in REM sleep originate mainly in dorsolateral portions of the pontine reticular formation, descend through the medulla and spinal cord, and inhibit the motoneurons in the brainstem and spinal cord, bringing about postural atonia. Cataplexy and sleep paralysis are pathological, dissociated manifestations of the generalized muscle atonia characteristic REM sleep. Cataplexy is triggered by emotional stimuli, probably through activation of the neural structure generating the muscle atonia of REM sleep. During long-lasting cataplectic attacks, narcoleptic humans often experience sleep paralysis and vivid hypnagogic hallucinations in the latter sleep state. Sleep paralysis is caused by the marked dissociation between level of alertness and muscle atonia that often occurs in SOREM sleep episodes. Frequent SOREM sleep episodes in narcoleptic humans and dogs may occur when some of the neural mechanisms producing wakefulness and/or NREM sleep that normally inhibit the occurrence of REM sleep are abnormally weak, or when neural mechanisms facilitating the occurrence of REM sleep are hypersensitive or hyperactive, or both. Both abnormalities may contribute to the occurrence of SOREM sleep episodes and sleep paralysis, and also to the emotional triggering of cataplexy. Frequent occurrence of SOREM sleep episodes seems to be prerequisite but not sufficient for the occurrence of cataplexy. Some additional neural activities induced by emotion also contribute by inhibiting and/or activating the disturbed neural mechanisms related to SOREM sleep episodes. These abnormalities in neural mechanisms probably involve hypersensitivity or hyperactivity of muscarinic cholinergic and/or cholinoceptive neuronal populations in the pontine and suprapontine structures, and/or abnormally decreased activity of noradrenergic or serotonergic neuronal populations in the pons and/or other brainstem structures. This last monoaminergic neuronal population probably has a gating or inhibiting effect upon the cholinergic and cholinoceptive neuronal populations related to the generation of generalized muscle atonia and REM sleep. In spite of many studies and published reports on REM sleep, as well as on cataplexy and sleep paralysis, we are still far from a complete understanding of the physiological mechanisms producing muscle atonia in REM sleep and of the pathophysiological mechanisms of cataplexy and sleep paralysis--though it is apparent that these mechanisms are closely related.</em></small><br>[ID: 8848973]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.beproc.2016.04.015>Heart rate variability predicts the emotional state in dogs.</a></strong><br>2016. Katayama M, Kubo T, Mogi K, Ikeda K, Nagasawa M, Kikusui T<br><small><strong>Abstract:</strong> <em>Although it is known that heart rate variability (HRV) is a useful indicator of emotional states in animals, there are few reports of research in dogs. Thus, we investigated the relationship between HRV and emotional states in dogs. The electrocardiogram and behavior in two situations that elicited a positive and negative emotion, in addition to baseline (when dogs were not presented any social stimuli), were recorded in 33 healthy house dogs. After testing, we chose 15seconds from each situation and baseline and calculated three HRV parameters: standard deviation of normal-to-normal R-R intervals (SDNN), the root mean square of successive heartbeat interval differences (RMSSD), and mean R-R intervals (mean RRI). In comparing these parameters with baseline, only SDNN was lower in a positive situation. In contrast, only RMSSD was lower in a negative situation. A change in HRV occurred with a stimulus eliciting emotion, and was able to distinguish between positive and negative situations. Thus, HRV is useful for estimating the emotional state in dogs. </em></small><br>[ID: 27129806]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-021-01471-x>Bodily emotional expressions are a primary source of information for dogs, but not for humans.</a></strong><br>2021. Correia-Caeiro C, Guo K, Mills D<br><small><strong>Abstract:</strong> <em>Dogs have remarkable abilities to synergise their behaviour with that of people, but how dogs read facial and bodily emotional cues in comparison to humans remains unclear. Both species share the same ecological niche, are highly social and expressive, making them an ideal comparative model for intra- and inter-species emotion perception. We compared eye-tracking data from unrestrained humans and dogs when viewing dynamic and naturalistic emotional expressions in humans and dogs. Dogs attended more to the body than the head of human and dog figures, unlike humans who focused more on the head of both species. Dogs and humans also showed a clear age effect that reduced head gaze. Our results indicate a species-specific evolutionary adaptation for emotion perception, which is only partly modified for heterospecific cues. These results have important implications for managing the risk associated with human-dog interactions, where expressive and perceptual differences are crucial.</em></small><br>[ID: 33507407]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0094597>Jealousy in dogs.</a></strong><br>2014. Harris CR, Prouvost C<br><small><strong>Abstract:</strong> <em>It is commonly assumed that jealousy is unique to humans, partially because of the complex cognitions often involved in this emotion. However, from a functional perspective, one might expect that an emotion that evolved to protect social bonds from interlopers might exist in other social species, particularly one as cognitively sophisticated as the dog. The current experiment adapted a paradigm from human infant studies to examine jealousy in domestic dogs. We found that dogs exhibited significantly more jealous behaviors (e.g., snapping, getting between the owner and object, pushing/touching the object/owner) when their owners displayed affectionate behaviors towards what appeared to be another dog as compared to nonsocial objects. These results lend support to the hypothesis that jealousy has some \"primordial\" form that exists in human infants and in at least one other social species besides humans. </em></small><br>[ID: 25054800]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1098/rsbl.2015.0883>Dogs recognize dog and human emotions.</a></strong><br>2016. Albuquerque N, Guo K, Wilkinson A, Savalli C, Otta E, Mills D<br><small><strong>Abstract:</strong> <em>The perception of emotional expressions allows animals to evaluate the social intentions and motivations of each other. This usually takes place within species; however, in the case of domestic dogs, it might be advantageous to recognize the emotions of humans as well as other dogs. In this sense, the combination of visual and auditory cues to categorize others' emotions facilitates the information processing and indicates high-level cognitive representations. Using a cross-modal preferential looking paradigm, we presented dogs with either human or dog faces with different emotional valences (happy/playful versus angry/aggressive) paired with a single vocalization from the same individual with either a positive or negative valence or Brownian noise. Dogs looked significantly longer at the face whose expression was congruent to the valence of vocalization, for both conspecifics and heterospecifics, an ability previously known only in humans. These results demonstrate that dogs can extract and integrate bimodal sensory emotional information, and discriminate between positive and negative emotions from both humans and dogs. </em></small><br>[ID: 26763220]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3758/s13420-018-0325-2>Orienting asymmetries and physiological reactivity in dogs' response to human emotional faces.</a></strong><br>2018. Siniscalchi M, d'Ingeo S, Quaranta A<br><small><strong>Abstract:</strong> <em>Recent scientific literature shows that emotional cues conveyed by human vocalizations and odours are processed in an asymmetrical way by the canine brain. In the present study, during feeding behaviour, dogs were suddenly presented with 2-D stimuli depicting human faces expressing the Ekman's six basic emotion (e.g. anger, fear, happiness, sadness, surprise, disgust, and neutral), simultaneously into the left and right visual hemifields. A bias to turn the head towards the left (right hemisphere) rather than the right side was observed with human faces expressing anger, fear, and happiness emotions, but an opposite bias (left hemisphere) was observed with human faces expressing surprise. Furthermore, dogs displayed higher behavioural and cardiac activity to picture of human faces expressing clear arousal emotional state. Overall, results demonstrated that dogs are sensitive to emotional cues conveyed by human faces, supporting the existence of an asymmetrical emotional modulation of the canine brain to process basic human emotions.</em></small><br>[ID: 29923158]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-017-15091-4>Dogs and humans respond to emotionally competent stimuli by producing different facial actions.</a></strong><br>2017. Caeiro C, Guo K, Mills D<br><small><strong>Abstract:</strong> <em>The commonality of facial expressions of emotion has been studied in different species since Darwin, with most of the research focusing on closely related primate species. However, it is unclear to what extent there exists common facial expression in species more phylogenetically distant, but sharing a need for common interspecific emotional understanding. Here we used the objective, anatomically-based tools, FACS and DogFACS (Facial Action Coding Systems), to quantify and compare human and domestic dog facial expressions in response to emotionally-competent stimuli associated with different categories of emotional arousal. We sought to answer two questions: Firstly, do dogs display specific discriminatory facial movements in response to different categories of emotional stimuli? Secondly, do dogs display similar facial movements to humans when reacting in emotionally comparable contexts? We found that dogs displayed distinctive facial actions depending on the category of stimuli. However, dogs produced different facial movements to humans in comparable states of emotional arousal. These results refute the commonality of emotional expression across mammals, since dogs do not display human-like facial expressions. Given the unique interspecific relationship between dogs and humans, two highly social but evolutionarily distant species sharing a common environment, these findings give new insight into the origin of emotion expression.</em></small><br>[ID: 29138393]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fvets.2019.00499>Developing Diagnostic Frameworks in Veterinary Behavioral Medicine: Disambiguating Separation Related Problems in Dogs.</a></strong><br>2019. de Assis LS, Matos R, Pike TW, Burman OHP, Mills DS<br><small><strong>Abstract:</strong> <em>Diagnoses are widely used in both human and veterinary medicine to describe the nature of a condition; by contrast, syndromes are collections of signs that consistently occur together to form a characteristic presentation. Treatment of syndromes, due to either their lack of a clear biological cause or multiple causes, necessarily remains non-specific. However, the discovery of interventions may help refine the definition of a syndrome into a diagnosis. Within the field of veterinary behavioral medicine, separation related problems (SRPs) provide a good example of a syndrome. We describe here a comprehensive process to develop a diagnostic framework (including quality control assessments), for disambiguating the signs of SRPs as an example of a heterogeneous behavioral syndrome in non-human animals requiring greater diagnostic and treatment precision. To do this we developed an online questionnaire (243 items) that covered the full spectrum of theoretical bases to the syndrome and undertook a large-scale survey of the presenting signs of dogs with one or more of the signs of SRPs (</em></small><br>[ID: 32010714]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.beproc.2017.11.006>Mouth-licking by dogs as a response to emotional stimuli.</a></strong><br>2018. Albuquerque N, Guo K, Wilkinson A, Resende B, Mills DS<br><small><strong>Abstract:</strong> <em>Dogs are able to perceptually discriminate emotional displays of conspecifics and heterospecifics and possess the cognitive prototypes for emotional categorisation, however, it remains unclear whether dogs can respond appropriately to this information. One way to assess associations between specific behaviours and the perception of emotionally competent stimuli is to look at other reliable measures that are related to cognitive and physiological processing. Using a cross-modal preferential looking paradigm (Albuquerque et al., 2016), we presented dogs with pairs of facial expressions (positive and negative) combined with an emotionally charged vocalisation (positive or negative) or a control sound (neutral) and coded their mouth-licking behaviour. We found an effect of the valence of the face image dogs were seeing on the onset of the mouth-licking, with higher frequencies of this behaviour in response to the negative faces compared to images with positive valence. However, neither the sound being played nor the interaction between image valence and sound affected the behaviour. We also found an effect of species with mouth-licking occurring more often towards human stimuli. This spontaneous differential behavioural response, combined with previous evidence of cognitive emotional processing in these animals, suggests that dogs may have a functional understanding of emotional expressions.</em></small><br>[ID: 29129727]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-020-67092-5>REM versus Non-REM sleep disturbance specifically affects inter-specific emotion processing in family dogs (Canis familiaris).</a></strong><br>2020. Boll H, Kovcs K, Lefter R, Gombos F, Kubinyi E, Topl J, Kis A<br><small><strong>Abstract:</strong> <em>Dogs have outstanding capabilities to read human emotional expressions, both vocal and facial. It has also been shown that positively versus negatively valenced dog-human social interactions substantially affect dogs' subsequent sleep. In the present study, we manipulated dogs' (N=15, in a within subject design) sleep structure by specifically disrupting REM versus Non-REM sleep, while maintaining equal sleep efficiency (monitored via non-invasive polysomnography). We found that both the number of awakenings as well as relative Non-REM (but not relative REM) duration influenced dogs' viewing patterns in a task where sad and happy human faces were simultaneously projected with sad or happy human voice playbacks. In accordance with the emotion laterality hypothesis, the interaction between sound valence and Non-REM sleep duration was specific to images projected to the left (regardless of image-sound congruency). These results reveal the first evidence of a causal link between sleep structure and inter-specific emotion-processing in the family dog.</em></small><br>[ID: 32591578]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1098/rsos.170134>Dog growls express various contextual and affective content for human listeners.</a></strong><br>2017. Farag T, Takcs N, Miklsi , Pongrcz P<br><small><strong>Abstract:</strong> <em>Vocal expressions of emotions follow simple rules to encode the inner state of the caller into acoustic parameters, not just within species, but also in cross-species communication. Humans use these structural rules to attribute emotions to dog vocalizations, especially to barks, which match with their contexts. In contrast, humans were found to be unable to differentiate between playful and threatening growls, probably because single growls' aggression level was assessed based on acoustic size cues. To resolve this contradiction, we played back natural growl bouts from three social contexts (food guarding, threatening and playing) to humans, who had to rate the emotional load and guess the context of the playbacks. Listeners attributed emotions to growls according to their social contexts. Within threatening and playful contexts, bouts with shorter, slower pulsing growls and showing smaller apparent body size were rated to be less aggressive and fearful, but more playful and happy. Participants associated the correct contexts with the growls above chance. Moreover, women and participants experienced with dogs scored higher in this task. Our results indicate that dogs may communicate honestly their size and inner state in a serious contest situation, while manipulatively in more uncertain defensive and playful contexts.</em></small><br>[ID: 28573021]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.cub.2014.12.055>Dogs can discriminate emotional expressions of human faces.</a></strong><br>2015. Mller CA, Schmitt K, Barber AL, Huber L<br><small><strong>Abstract:</strong> <em>The question of whether animals have emotions and respond to the emotional expressions of others has become a focus of research in the last decade [1-9]. However, to date, no study has convincingly shown that animals discriminate between emotional expressions of heterospecifics, excluding the possibility that they respond to simple cues. Here, we show that dogs use the emotion of a heterospecific as a discriminative cue. After learning to discriminate between happy and angry human faces in 15 picture pairs, whereby for one group only the upper halves of the faces were shown and for the other group only the lower halves of the faces were shown, dogs were tested with four types of probe trials: (1) the same half of the faces as in the training but of novel faces, (2) the other half of the faces used in training, (3) the other half of novel faces, and (4) the left half of the faces used in training. We found that dogs for which the happy faces were rewarded learned the discrimination more quickly than dogs for which the angry faces were rewarded. This would be predicted if the dogs recognized an angry face as an aversive stimulus. Furthermore, the dogs performed significantly above chance level in all four probe conditions and thus transferred the training contingency to novel stimuli that shared with the training set only the emotional expression as a distinguishing feature. We conclude that the dogs used their memories of real emotional human faces to accomplish the discrimination task.</em></small><br>[ID: 25683806]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/0165-1838(89)90088-x>'Non-chronotropic' mechanisms on withdrawal of efferent vagal stimulation in anesthetized dogs.</a></strong><br>1989. Cevese A, Verlato G, Cerutti G<br><small><strong>Abstract:</strong> <em>Withdrawal of the efferent vagal tone to the heart is an important factor of the increase of cardiac output (CO) and arterial blood pressure (ABP) in several conditions, such as exercise, emotion, postural changes. Vagal withdrawal enhances cardiovascular performance both by increasing heart rate (HR) and by other mechanisms, which were globally named 'non-chronotropic mechanisms'. The nature of these non-chronotropic mechanisms was studied in open-chest dogs under morphine-chloralose anesthesia. After cutting the cervical vagi and all the branches of the stellate ganglia except for the ansae subclaviae, the animals were prepared for recording HR, ABP, CO and left ventricular pressure (LVP). The experiments started during control vagal stimulations and consisted either in turning the vagal stimulators off (STOP), or in raising HR by atrial placing without withdrawing vagal stimulation (PACE), or in turning the vagal stimulators off while keeping HR constant by atrial pacing since the control vagal stimulation (STPA). Thus, STOP, PACE and STPA produced withdrawal of all vagal effects, of the chronotropic effects and of the non-chronotropic effects, respectively. Non-chronotropic mechanisms were evaluated both as the effects of STPA and as the difference between the effects of STOP and PACE. Experiments were repeated during stellate ganglion stimulation and during simultaneous atrio-ventricular pacing, to evaluate the role of vagosympathetic interactions and of atrial contractility. CO increased by 25% after STOP, by 20% after PACE and by 5% after STPA in the absence of sympathetic stimulation and by 30% after STOP, by 20% after PACE and by 10% after STPA during sympathetic stimulation. Stellate ganglion stimulation doubled non-chronotropic effects probably by potentiating vagal effects on myocardial contractility: after STPA the maximum LVdP/dt increased by 2% without sympathetic stimulation and by 7% with sympathetic stimulation. In all conditions, the increases in ABP after STOP, PACE and STPA were small and not statistically different between STOP and PACE. Simultaneous atrio-ventricular pacing in the absence of sympathetic stimulation nearly abolished non-chronotropic mechanisms, since CO increased to about the same extent both with STOP and with PACE. It is concluded that non-chronotropic mechanisms on vagal withdrawal consist mainly in the enhancement of atrial contractility and in the release of vagal restraint on the sympathetic effects upon the ventricles.</em></small><br>[ID: 2625503]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1080/10888705.2021.1923493>Still-face Effect in Dogs (<i>Canis familiaris</i>). A Pilot Study.</a></strong><br>2021. Barrera G, Guilln-Salazar F, Bentosela M<br><small><strong>Abstract:</strong> <em>The Still-face Paradigm has been widely used for the assessment of emotion regulation in infants, as well as for the study of the mother-child relationship. Given the close bond that dogs have with humans, the purpose of this research was to evaluate, through an exploratory descriptive study, the presence of the Still-face effect in dogs. To this end, a group of Beagle dogs were exposed to three one-minute phases in which first, an unknown experimenter interacted actively and positively with each dog (Interaction). Then, suddenly, she interrupted the interaction and remained passive, with a non-expressive face and without speaking or petting the dog (Still-face). Finally, the experimenter reestablished the interaction (Reunion). Our results showed a decrease in affiliative behaviors in dogs during the Still-face phase according to changes in the human's behavior, a pattern similar to the one previously found in infants. Contrary to expectations, no stress-related behaviors were shown during that phase. A carry-over effect was also observed in the Reunion phase. This study provides information about the human-dog interaction and the effects of its disruption on dogs' behaviors.</em></small><br>[ID: 33988060]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/s41598-020-76806-8>Time-resolved classification of dog brain signals reveals early processing of faces, species and emotion.</a></strong><br>2020. Kujala MV, Kauppi JP, Trnqvist H, Helle L, Vainio O, Kujala J, Parkkonen L<br><small><strong>Abstract:</strong> <em>Dogs process faces and emotional expressions much like humans, but the time windows important for face processing in dogs are largely unknown. By combining our non-invasive electroencephalography (EEG) protocol on dogs with machine-learning algorithms, we show category-specific dog brain responses to pictures of human and dog facial expressions, objects, and phase-scrambled faces. We trained a support vector machine classifier with spatiotemporal EEG data to discriminate between responses to pairs of images. The classification accuracy was highest for humans or dogs vs. scrambled images, with most informative time intervals of 100-140ms and 240-280ms. We also detected a response sensitive to threatening dog faces at 30-40ms; generally, responses differentiating emotional expressions were found at 130-170ms, and differentiation of faces from objects occurred at 120-130ms. The cortical sources underlying the highest-amplitude EEG signals were localized to the dog visual cortex.</em></small><br>[ID: 33199715]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fvets.2018.00103>A Systematic Review of the Reliability and Validity of Behavioural Tests Used to Assess Behavioural Characteristics Important in Working Dogs.</a></strong><br>2018. Brady K, Cracknell N, Zulch H, Mills DS<br><small><strong>Abstract:</strong> <em>Working dogs are selected based on predictions from tests that they will be able to perform specific tasks in often challenging environments. However, withdrawal from service in working dogs is still a big problem, bringing into question the reliability of the selection tests used to make these predictions.</em></small><br>[ID: 29888234]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0003349>Hemispheric specialization in dogs for processing different acoustic stimuli.</a></strong><br>2008. Siniscalchi M, Quaranta A, Rogers LJ<br><small><strong>Abstract:</strong> <em>Considerable experimental evidence shows that functional cerebral asymmetries are widespread in animals. Activity of the right cerebral hemisphere has been associated with responses to novel stimuli and the expression of intense emotions, such as aggression, escape behaviour and fear. The left hemisphere uses learned patterns and responds to familiar stimuli. Although such lateralization has been studied mainly for visual responses, there is evidence in primates that auditory perception is lateralized and that vocal communication depends on differential processing by the hemispheres. The aim of the present work was to investigate whether dogs use different hemispheres to process different acoustic stimuli by presenting them with playbacks of a thunderstorm and their species-typical vocalizations. The results revealed that dogs usually process their species-typical vocalizations using the left hemisphere and the thunderstorm sounds using the right hemisphere. Nevertheless, conspecific vocalizations are not always processed by the left hemisphere, since the right hemisphere is used for processing vocalizations when they elicit intense emotion, including fear. These findings suggest that the specialisation of the left hemisphere for intraspecific communication is more ancient that previously thought, and so is specialisation of the right hemisphere for intense emotions.</em></small><br>[ID: 18843371]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2019.01678>Emotional Contagion From Humans to Dogs Is Facilitated by Duration of Ownership.</a></strong><br>2019. Katayama M, Kubo T, Yamakawa T, Fujiwara K, Nomoto K, Ikeda K, Mogi K, Nagasawa M, Kikusui T<br><small><strong>Abstract:</strong> <em>Emotional contagion is a primitive form of empathy that does not need higher psychological functions. Recent studies reported that emotional contagion exists not only between humans but also among various animal species. The dog (</em></small><br>[ID: 31379690]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1007/s10071-015-0927-4>Man's other best friend: domestic cats (F. silvestris catus) and their discrimination of human emotion cues.</a></strong><br>2016. Galvan M, Vonk J<br><small><strong>Abstract:</strong> <em>The ability of domestic dogs (C. lupus famaliaris) to follow and attend to human emotion expressions is well documented. It is unknown whether domestic cats (F. silvestris catus) possess similar abilities. Because cats belong to the same order (Carnivora), but did not evolve to live in complex social groups, research with them enables us to tease apart the influence of social structure versus domestication processes on the capacity to recognize human communicative cues, such as emotions. Two experiments were conducted to determine the extent to which domestic cats discriminate between human emotion cues. The first experiment presented cats with facial and postural cues of happiness and anger from both an unfamiliar experimenter and their familiar owner in the absence of vocal cues. The second experiment presented cats with vocal cues of human emotion through a positively or negatively charged conversation between an experimenter and owner. Domestic cats were only modestly sensitive to emotion, particularly when displayed by their owner, suggesting that a history of human interaction alone may not be sufficient to shape such abilities in domestic cats. </em></small><br>[ID: 26400749]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.bbr.2016.02.011>The dog nose \"KNOWS\" fear: Asymmetric nostril use during sniffing at canine and human emotional stimuli.</a></strong><br>2016. Siniscalchi M, d'Ingeo S, Quaranta A<br><small><strong>Abstract:</strong> <em>Previous studies have reported striking asymmetries in the nostril use of dogs during sniffing at different emotive stimuli. Here we report, for the first time, that this asymmetry is also manifested during sniffing of both human and canine odours collected during different emotional events. Results showed that during sniffing of conspecific odour collected during a stressful situation (e.g. an \"isolation\" situation in which a dog was isolated from its owner in an unfamiliar environment) dogs consistently used their right nostril (right hemisphere). On the other hand, dogs consistently used the left nostril to sniff human odours collected during fearful situations (emotion-eliciting movies) and physical stress, suggesting the prevalent activation of the left hemisphere. The opposite bias shown in nostril use during sniffing at canine versus human odours suggests that chemosignals communicate conspecific and heterospecific emotional cues using different sensory pathways. </em></small><br>[ID: 26876141]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0170397>Is your dog empathic? Developing a Dog Emotional Reactivity Survey.</a></strong><br>2017. Sznth F, Miklsi , Kubinyi E<br><small><strong>Abstract:</strong> <em>Dogs' seemingly empathic behaviour attracts general and scientific attention alike. Behaviour tests are usually not sufficiently realistic to evoke empathic-like behaviour; therefore we decided to ask owners about their experiences with their dogs in emotionally loaded situations. Owners from Hungary (N = 591) and from Germany (N = 2283) were asked to rate their level of agreement on a 1-5 Likert scale with statements about the reactivity of their dogs to their emotions and to other dogs' behaviour. We created two scales with satisfactory internal reliability: reactivity to the owner's emotion and reactivity to other dogs' behaviour. Based on an owner-dog personality matching theory, we hypothesised that the owner's empathy, as measured by the subscale on the cooperativeness character factor of the human personality, will correlate with their dog's emotional reactivity in emotionally loaded situations. In addition we also examined how anthropomorphism, contagious yawning, attitude toward the dog are related to emotional reactivity in dogs as perceived by the owner. In addition we examined how owners rate dog pictures. We found that the scale scores were largely independent from demographic and environmental variables like breed, sex, age, age at acquiring, keeping practices, training experiences and owner's age. However, anthropomorphic and emotional attitude of the owners probably biased the responses. In the German sample more empathic owners reported to have more emotionally reactive dog, as expected by the personality matching theory. More empathic owners reported to have fewer problems with their dogs and they rated a puppy picture as more cute in both countries. 62% of owners from Hungary and 36% of owner from Germany agreed with the statement \"My dog is more important for me than any human being\". In Germany, more empathic owners agreed less with this statement and indicated that their dogs have a tendency for contagious yawning. Owners whose attitudes toward their dogs were anthropomorphic (agreed more with the statement that \"My dog thinks like a child\"), perceived their dogs as more reactive to their emotions. This findings highlights the importance of testing the attitudes of the respondents when they assess the personality and the emotions of animals. The criterion validity of the Dog Emotional Reactivity Survey should be confirmed by objective behavioural tests.</em></small><br>[ID: 28192495]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/ani8070107>Heart Rate and Heart Rate Variability during Sleep in Family Dogs (<i>Canis familiaris</i>). Moderate Effect of Pre-Sleep Emotions.</a></strong><br>2018. Varga B, Gergely A, Galambos , Kis A<br><small><strong>Abstract:</strong> <em>The domestic dog (</em></small><br>[ID: 30004461]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0114207>Emotion attribution to a non-humanoid robot in different social situations.</a></strong><br>2014. Lakatos G, Gcsi M, Konok V, Brder I, Bereczky B, Korondi P, Miklsi <br><small><strong>Abstract:</strong> <em>In the last few years there was an increasing interest in building companion robots that interact in a socially acceptable way with humans. In order to interact in a meaningful way a robot has to convey intentionality and emotions of some sort in order to increase believability. We suggest that human-robot interaction should be considered as a specific form of inter-specific interaction and that human-animal interaction can provide a useful biological model for designing social robots. Dogs can provide a promising biological model since during the domestication process dogs were able to adapt to the human environment and to participate in complex social interactions. In this observational study we propose to design emotionally expressive behaviour of robots using the behaviour of dogs as inspiration and to test these dog-inspired robots with humans in inter-specific context. In two experiments (wizard-of-oz scenarios) we examined humans' ability to recognize two basic and a secondary emotion expressed by a robot. In Experiment 1 we provided our companion robot with two kinds of emotional behaviour (\"happiness\" and \"fear\"), and studied whether people attribute the appropriate emotion to the robot, and interact with it accordingly. In Experiment 2 we investigated whether participants tend to attribute guilty behaviour to a robot in a relevant context by examining whether relying on the robot's greeting behaviour human participants can detect if the robot transgressed a predetermined rule. Results of Experiment 1 showed that people readily attribute emotions to a social robot and interact with it in accordance with the expressed emotional behaviour. Results of Experiment 2 showed that people are able to recognize if the robot transgressed on the basis of its greeting behaviour. In summary, our findings showed that dog-inspired behaviour is a suitable medium for making people attribute emotional states to a non-humanoid robot. </em></small><br>[ID: 25551218]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1121/1.1398052>The harmonic-to-noise ratio applied to dog barks.</a></strong><br>2001. Riede T, Herzel H, Hammerschmidt K, Brunnberg L, Tembrock G<br><small><strong>Abstract:</strong> <em>Dog barks are typically a mixture of regular components and irregular (noisy) components. The regular part of the signal is given by a series of harmonics and is most probably due to regular vibrations of the vocal folds, whereas noise refers to any nonharmonic (irregular) energy in the spectrum of the bark signal. The noise components might be due to chaotic vibrations of the vocal-fold tissue or due to turbulence of the air. The ratio of harmonic to nonharmonic energy in dog barks is quantified by applying the harmonics-to-noise ratio (HNR). Barks of a single dog breed were recorded in the same behavioral context. Two groups of dogs were considered: a group of ten healthy dogs (the normal sample), and a group of ten unhealthy dogs, i.e., dogs treated in a veterinary clinic (the clinic sample). Although the unhealthy dogs had no voice disease, differences in emotion or pain or impacts of surgery might have influenced their barks. The barks of the dogs were recorded for a period of 6 months. The HNR computation is based on the Fourier spectrum of a 50-ms section from the middle of the bark. A 10-point moving average curve of the spectrum on a logarithmic scale is considered as estimator of the noise level in the bark, and the maximum difference of the original spectrum and the moving average is defined as the HNR measure. It is shown that a reasonable ranking of the voices is achievable based on the measurement of the HNR. The HNR-based classification is found to be consistent with perceptual evaluation of the barks. In addition, a multiparametric approach confirms the classification based on the HNR. Hence, it may be concluded that the HNR might be useful as a novel parameter in bioacoustics for quantifying the noise within a signal.</em></small><br>[ID: 11681395]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2020.588916>Human Expressions of Object Preference Affect Dogs' Perceptual Focus, but Not Their Action Choices.</a></strong><br>2020. Kubinyi E, Sznth F, Gilmert E, Iotchev IB, Miklsi <br><small><strong>Abstract:</strong> <em>Inspired by work on infants, we investigated whether dogs' behaviors are guided by human displays of preference, contrasting with the animals' own choices. In a rewarded fetching task, dogs override their own interest toward \"disgusting\" objects and retrieve what the owner prefers. However, in previous research, both objects were inherently neutral to the dogs and they might have chosen the owner's object because a \"happy owner\" predicts a positive outcome. If dogs are indeed able to override their own interests, we expected them to fetch the owner's object even if (1) they would prefer another one and (2) do not receive a reward for it. Two objects were compared, a toy (hoop) and a bracelet. After establishing that the toy was preferred by all dogs in an initial test of preference, we applied a two-choice procedure to test if either fetching or looking at the objects from a distance would be affected by the owner's choice. In Study 1, the owner demonstrated happiness toward the bracelet and disgust toward the toy with both facial and body gestures accompanied by verbalizations. Then the owner asked the dog to fetch, without providing additional guiding cues. All dogs fetched the toy, indicating that their own choice was not overcome by the positive emotional state signaled by the owner. To avoid direct contact with the objects, in Study 2 we placed the objects on an unreachable spot after the emotion demonstration and measured the duration of looking at the objects. In the \"bracelet\" (non-matching) group the owners demonstrated happiness toward the bracelet and disgust toward the toy, similar to Study 1. In the \"toy\" (matching) group the owners showed happiness toward the toy and disgust toward the bracelet. When the objects were placed on the unreachable spot, dogs looked at both objects for the same amount of time in the non-matching group, but longer at the toy in the matching group. Although the studies did not demonstrate that dogs override their own preferences for an object, the results suggested that the owners' expressed preference was perceived by the dogs and guided their perceptual focus.</em></small><br>[ID: 33240181]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>[The role of the emotiogenically negative brain structures in realizing an acid-defensive instrumental motor reaction].</strong><br>1993. Davydova EK, Grigor'ian GA<br><small><strong>Abstract:</strong> <em>In experiments in dogs it has been found that testing electrical stimulation of the lateral hypothalamus (duration of this stimulation--5-7 s), of the ventromedial hypothalamus and of the substantia grisea centralis of mesencephalon reproduced earlier elaborated acid-defensive instrumental conditioned reflex. This fact takes place due to the activation of the brain structures of negative emotion.</em></small><br>[ID: 8385399]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.physbeh.2015.11.027>Assessing positive emotional states in dogs using heart rate and heart rate variability.</a></strong><br>2016. Zupan M, Buskas J, Altimiras J, Keeling LJ<br><small><strong>Abstract:</strong> <em>Since most animal species have been recognized as sentient beings, emotional state may be a good indicator of welfare in animals. The goal of this study was to manipulate the environment of nine beagle research dogs to highlight physiological responses indicative of different emotional experiences. Stimuli were selected to be a more or a less positive food (meatball or food pellet) or social reward (familiar person or less familiar person). That all the stimuli were positive and of different reward value was confirmed in a runway motivation test. Dogs were tested individually while standing facing a display theatre where the different stimuli could be shown by lifting a shutter. The dogs approached and remained voluntarily in the test system. They were tested in four sessions (of 20s each) for each of the four stimuli. A test session consisted of four presentation phases (1st exposure to stimulus, post exposure, 2nd exposure, and access to reward). Heart rate (HR) and heart rate variability (HRV) responses were recorded during testing in the experimental room and also when lying resting in a quiet familiar room. A new method of 'stitching' short periods of HRV data together was used in the analysis. When testing different stimuli, no significant differences were observed in HR and LF:HF ratio (relative power in low frequency (LF) and the high-frequency (HF) range), implying that the sympathetic tone was activated similarly for all the stimuli and may suggest that dogs were in a state of positive arousal. A decrease of HF was associated with the meatball stimulus compared to the food pellet and the reward phase (interacting with the person or eating the food) was associated with a decrease in HF and RMSSD (root mean square of successive differences of inter-beat intervals) compared to the preceding phase (looking at the person or food). This suggests that parasympathetic deactivation is associated with a more positive emotional state in the dog. A similar reduction in HF and RMSSD was found in the test situation compared to the resting situation. This is congruent with the expected autonomic effects related to postural shift i.e. sympathetic activation and parasympathetic withdrawal, during standing versus lying, but it cannot explain the parasympathetic deactivation in response to the more positive stimuli since the dogs were always standing in the test situation. We discuss the systematic pattern of responses, which support that increased HR and LF:HF ratio are associated with emotional arousal, but add the new proposal that a combined decrease in RMSSD and HF may reflect a more positively valenced emotional state even when an individual is already in a positive psychological state. </em></small><br>[ID: 26631546]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1017/S1041610211000226>Animal-assisted activity and emotional status of patients with Alzheimer's disease in day care.</a></strong><br>2011. Mossello E, Ridolfi A, Mello AM, Lorenzini G, Mugnai F, Piccini C, Barone D, Peruzzi A, Masotti G, Marchionni N<br><small><strong>Abstract:</strong> <em>Preliminary studies suggest beneficial effects of animal-assisted activities (AAA) on behavioral and psychological symptoms of dementia (BPSD), but data are inconsistent. This study aimed to assess the effect of AAA with dogs on cognition, BPSD, emotional status and motor activity in severe Alzheimer's disease (AD).</em></small><br>[ID: 21356158]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2017.01854>Nasal Oxytocin Treatment Biases Dogs' Visual Attention and Emotional Response toward Positive Human Facial Expressions.</a></strong><br>2017. Somppi S, Trnqvist H, Topl J, Koskela A, Hnninen L, Krause CM, Vainio O<br><small><strong>Abstract:</strong> <em>The neuropeptide oxytocin plays a critical role in social behavior and emotion regulation in mammals. The aim of this study was to explore how nasal oxytocin administration affects gazing behavior during emotional perception in domestic dogs. Looking patterns of dogs, as a measure of voluntary attention, were recorded during the viewing of human facial expression photographs. The pupil diameters of dogs were also measured as a physiological index of emotional arousal. In a placebo-controlled within-subjects experimental design, 43 dogs, after having received either oxytocin or placebo (saline) nasal spray treatment, were presented with pictures of unfamiliar male human faces displaying either a happy or an angry expression. We found that, depending on the facial expression, the dogs' gaze patterns were affected selectively by oxytocin treatment. After receiving oxytocin, dogs fixated less often on the eye regions of angry faces and revisited (glanced back at) more often the eye regions of smiling (happy) faces than after the placebo treatment. Furthermore, following the oxytocin treatment dogs fixated and revisited the eyes of happy faces significantly more often than the eyes of angry faces. The analysis of dogs' pupil diameters during viewing of human facial expressions indicated that oxytocin may also have a modulatory effect on dogs' emotional arousal. While subjects' pupil sizes were significantly larger when viewing angry faces than happy faces in the control (placebo treatment) condition, oxytocin treatment not only eliminated this effect but caused an opposite pupil response. Overall, these findings suggest that nasal oxytocin administration selectively changes the allocation of attention and emotional arousal in domestic dogs. Oxytocin has the potential to decrease vigilance toward threatening social stimuli and increase the salience of positive social stimuli thus making eye gaze of friendly human faces more salient for dogs. Our study provides further support for the role of the oxytocinergic system in the social perception abilities of domestic dogs. We propose that oxytocin modulates fundamental emotional processing in dogs through a mechanism that may facilitate communication between humans and dogs.</em></small><br>[ID: 29089919]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def query_OR(query):\n",
    "    tokens = preprocess(tokenize(query))\n",
    "    result_docs = list(set(inverted_index[tokens[0]]))\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in inverted_index:\n",
    "            result_docs = merge_OR(result_docs, list(set(inverted_index[token])))\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "or_query3 = \"enlighten dogs\"\n",
    "matched_3 = query_OR(or_query3)\n",
    "for doc_id in matched_3:\n",
    "    display_summary(doc_id, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Why does `query_and('visual complex scenes emotion')` not return our example paper 31588367, even though it mentions these terms in the title and abstract? (You do not have to implement anything to fix this yet!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "The inability of query_AND('visual complex scenes emotion') to retrieve the  paper mentioned demonstrates several limitations in both the indexing strategy and the basic nature of the tokenize and preprocess functions. \n",
    "The tokenization method, which splits text on spaces, fails to address language's intricacies, like punctuation, compound words, and phrases. Consequently, it may not represent the texts context or meaning accurately. The preprocessing is very basic, limited to lowercasing tokens without incorporating essential linguistic processes like stemming, lemmatization, or stopword removal.\n",
    "\n",
    "Furthermore, the inverted index, configured with a defaultdict, lacks sophisticated data structures like B-trees or hash tables, leading to less efficient term lookup. The absence of support for wildcard, fuzzy searches, or spelling correction in the query processing exacerbates the issue by failing to match term variations or misspellings. Additionally, the indexing methodology, mainly aggregating terms from titles and abstracts, does not account for dynamic updates or distributed indexing, potentially overlooking recent document changes or relevant terms. These combined factors limit the system's accuracy in matching and retrieving documents, especially for complex queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done **individually**, and that code sharing or copying are **strictly forbidden** and will be punished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
